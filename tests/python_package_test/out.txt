============================= test session starts ==============================
platform linux -- Python 3.8.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1
rootdir: /opt/LightGBM/tests/python_package_test
collected 81 items

test_dask.py xFFFFFFFFFFFFFFFFFFF.FFFFFFFF..FFFFF..FFFFxFFFFFFFFFFFFFFFF [ 72%]
FFFF.FFFFF.FFFFF..FFFF                                                   [100%]

=================================== FAILURES ===================================
____ test_sklearn_integration[DaskLGBMClassifier()-check_estimators_dtypes] ____

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_dtypes at 0x7f87759d4040>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:38511' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1408: in check_estimators_dtypes
    estimator.fit(X_train, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.6464405 , 2.145568  , 1.80829   , 1.6346495 , 1.2709644 ],
       [1.9376824 , 1.3127615 , 2.675319  , 2.890...6 , 2.1489816 , 0.8682183 ],
       [0.5495741 , 1.7595388 , 0.06032264, 2.4868202 , 0.01408643]],
      dtype=float32)
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:38511
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39881
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39881
distributed.worker - INFO -          dashboard at:            127.0.0.1:41645
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38511
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-a8c5a34f-0c3e-4f75-ae55-bca966617bf3/dask-worker-space/worker-ufnyqqwd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40325
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40325
distributed.worker - INFO -          dashboard at:            127.0.0.1:36351
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38511
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-3c1eeaba-716e-4047-a497-eae03c463274/dask-worker-space/worker-4khfka0v
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39881', name: tcp://127.0.0.1:39881, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39881
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38511
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40325', name: tcp://127.0.0.1:40325, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38511
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40325
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-3dcacc49-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-3dcacc49-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-3dcacc49-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-3dcacc49-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40325
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39881
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39881', name: tcp://127.0.0.1:39881, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39881
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40325', name: tcp://127.0.0.1:40325, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40325
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
____ test_sklearn_integration[DaskLGBMClassifier()-check_fit_score_takes_y] ____

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit_score_takes_y at 0x7f87759d0ee0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:43443' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1380: in check_fit_score_takes_y
    func(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135 , 0.71518937, 0.60276338],
       [0.54488318, 0.4236548 , 0.64589411],
       [0.43758721, 0.891773...99, 0.0641475 , 0.69247212],
       [0.56660145, 0.26538949, 0.52324805],
       [0.09394051, 0.5759465 , 0.9292962 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43443
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34695
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34695
distributed.worker - INFO -          dashboard at:            127.0.0.1:41163
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43443
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-0c63f328-750f-4f27-b435-dd9d57659dc0/dask-worker-space/worker-keu9tqg9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40165
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40165
distributed.worker - INFO -          dashboard at:            127.0.0.1:34655
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43443
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-02df6e40-8c2a-491b-8516-2b5560bce91d/dask-worker-space/worker-xna7ykj4
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34695', name: tcp://127.0.0.1:34695, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34695
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43443
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40165', name: tcp://127.0.0.1:40165, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40165
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43443
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-40675086-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-40675086-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-40675086-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-40675086-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34695
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34695', name: tcp://127.0.0.1:34695, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40165
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40165', name: tcp://127.0.0.1:40165, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40165
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_pandas_series] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_pandas_series at 0x7f87759ce820>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:37277' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:788: in check_sample_weights_pandas_series
    estimator.fit(X, y, sample_weight=weights)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: in _split_to_parts
    parts = data.to_delayed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self =     0  1
0   1  1
1   1  2
2   1  3
3   1  4
4   2  1
5   2  2
6   2  3
7   2  4
8   3  1
9   3  2
10  3  3
11  3  4
name = 'to_delayed'

    def __getattr__(self, name: str):
        """
        After regular attribute access, try looking up the name
        This allows simpler access to columns for interactive use.
        """
        # Note: obj.x will always call obj.__getattribute__('x') prior to
        # calling obj.__getattr__('x').
        if (
            name in self._internal_names_set
            or name in self._metadata
            or name in self._accessors
        ):
            return object.__getattribute__(self, name)
        else:
            if self._info_axis._can_hold_identifiers_and_holds_name(name):
                return self[name]
>           return object.__getattribute__(self, name)
E           AttributeError: 'DataFrame' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py:5139: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:37277
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35059
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35059
distributed.worker - INFO -          dashboard at:            127.0.0.1:36303
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37277
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-4e17b785-3a07-4a1a-ae0f-833f7043a9dc/dask-worker-space/worker-38onnsn3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44055
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44055
distributed.worker - INFO -          dashboard at:            127.0.0.1:39983
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37277
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-c1f54cc9-6f5b-4a65-95e9-94c158d316e2/dask-worker-space/worker-9z7vn77y
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35059', name: tcp://127.0.0.1:35059, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35059
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37277
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44055', name: tcp://127.0.0.1:44055, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44055
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37277
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-42e3148a-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-42e3148a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-42e3148a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-42e3148a-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35059
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35059', name: tcp://127.0.0.1:35059, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35059
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44055
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44055', name: tcp://127.0.0.1:44055, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44055
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_not_an_array] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_not_an_array at 0x7f87759ce940>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:35431' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:812: in check_sample_weights_not_an_array
    estimator.fit(X, y, sample_weight=weights)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = <sklearn.utils.estimator_checks._NotAnArray object at 0x7f877409fbe0>
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: '_NotAnArray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:35431
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35095
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35095
distributed.worker - INFO -          dashboard at:            127.0.0.1:42963
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35431
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-dc4ef792-27bb-4c53-80e6-feaa2758d54f/dask-worker-space/worker-789g4296
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37011
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37011
distributed.worker - INFO -          dashboard at:            127.0.0.1:32851
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35431
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-f96b5386-66b9-4438-9db9-be8128603e8c/dask-worker-space/worker-q0u0m7b5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35095', name: tcp://127.0.0.1:35095, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35095
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35431
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37011', name: tcp://127.0.0.1:37011, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37011
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35431
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-4561fe70-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-4561fe70-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-4561fe70-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-4561fe70-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35095
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37011
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35095', name: tcp://127.0.0.1:35095, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35095
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37011', name: tcp://127.0.0.1:37011, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37011
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_list] ___

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_list at 0x7f87759cea60>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:46709' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:829: in check_sample_weights_list
    estimator.fit(X, y, sample_weight=sample_weight)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135 , 0.71518937, 0.60276338],
       [0.54488318, 0.4236548 , 0.64589411],
       [0.43758721, 0.891773...99, 0.0641475 , 0.69247212],
       [0.56660145, 0.26538949, 0.52324805],
       [0.09394051, 0.5759465 , 0.9292962 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:46709
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43957
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43957
distributed.worker - INFO -          dashboard at:            127.0.0.1:43339
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46709
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-1f0578ea-b207-4a35-8193-3b132b6ec99f/dask-worker-space/worker-67k57_sg
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:43957', name: tcp://127.0.0.1:43957, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43957
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46709
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36339
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36339
distributed.worker - INFO -          dashboard at:            127.0.0.1:44903
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46709
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-9a935d68-c29e-4d83-b039-ad70ae57f57f/dask-worker-space/worker-19tosbh_
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36339', name: tcp://127.0.0.1:36339, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36339
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46709
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-47e8b99a-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-47e8b99a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-47e8b99a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-47e8b99a-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43957
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36339
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:43957', name: tcp://127.0.0.1:43957, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:43957
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36339', name: tcp://127.0.0.1:36339, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36339
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
__ test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_shape] ___

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_shape at 0x7f87759ceb80>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:37331' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:848: in check_sample_weights_shape
    estimator.fit(X, y, sample_weight=np.ones(len(y)))
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1, 3],
       [1, 3],
       [1, 3],
       [1, 3],
       [2, 1],
       [2, 1],
       [2, 1],
       [2, 1],
       [3, 3],
       [3, 3],
       [3, 3],
       [3, 3],
       [4, 1],
       [4, 1],
       [4, 1],
       [4, 1]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:37331
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37229
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37229
distributed.worker - INFO -          dashboard at:            127.0.0.1:40645
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37331
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-47ddee10-d096-4e50-a821-1d9422c97f8c/dask-worker-space/worker-4ju_mnne
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37229', name: tcp://127.0.0.1:37229, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37229
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37331
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36883
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36883
distributed.worker - INFO -          dashboard at:            127.0.0.1:46265
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37331
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-4bdc1ed0-7dc3-4dca-80c4-4a3c0f161223/dask-worker-space/worker-mb_gibme
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36883', name: tcp://127.0.0.1:36883, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36883
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37331
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-4a68b645-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-4a68b645-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-4a68b645-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-4a68b645-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36883
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37229
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37229', name: tcp://127.0.0.1:37229, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37229
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36883', name: tcp://127.0.0.1:36883, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36883
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_invariance] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_invariance at 0x7f87759ceca0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:46639' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:879: in check_sample_weights_invariance
    estimator1.fit(X, y=y, sample_weight=np.ones(shape=len(y)))
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1., 3.],
       [1., 3.],
       [1., 3.],
       [1., 3.],
       [2., 1.],
       [2., 1.],
       [2., 1.],...       [3., 3.],
       [3., 3.],
       [3., 3.],
       [4., 1.],
       [4., 1.],
       [4., 1.],
       [4., 1.]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:46639
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44399
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44399
distributed.worker - INFO -          dashboard at:            127.0.0.1:45175
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46639
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-439e4079-ef8d-4bc4-88f5-0eb5f983a2d9/dask-worker-space/worker-_49s5zdw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41817
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41817
distributed.worker - INFO -          dashboard at:            127.0.0.1:43979
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46639
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-466f52a2-6182-432e-bbba-baaa4749903d/dask-worker-space/worker-7nw8b83v
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44399', name: tcp://127.0.0.1:44399, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44399
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:41817', name: tcp://127.0.0.1:41817, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46639
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46639
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41817
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-4cc07dc4-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-4cc07dc4-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-4cc07dc4-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-4cc07dc4-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41817
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44399
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:41817', name: tcp://127.0.0.1:41817, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:41817
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44399', name: tcp://127.0.0.1:44399, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44399
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_estimators_fit_returns_self] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_fit_returns_self at 0x7f87759d7040>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:34707' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2058: in check_estimators_fit_returns_self
    assert estimator.fit(X, y) is estimator
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[5.44338915, 4.50900038],
       [4.52251198, 6.68286579],
       [5.33420024, 3.94365401],
       [6.15287792,...65598, 2.1500414 ],
       [0.45347483, 6.92854682],
       [4.97048201, 7.65863655],
       [2.93656087, 7.35343631]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:34707
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44975
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44975
distributed.worker - INFO -          dashboard at:            127.0.0.1:33665
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34707
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-9632361a-ccf8-4922-bde5-c4aff680f507/dask-worker-space/worker-llj0fnbu
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44975', name: tcp://127.0.0.1:44975, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44975
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34707
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39859
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39859
distributed.worker - INFO -          dashboard at:            127.0.0.1:43109
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34707
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-2783fb0c-aabb-46e5-b342-5f9c52046ad0/dask-worker-space/worker-_z8muejj
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39859', name: tcp://127.0.0.1:39859, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39859
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34707
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-4f073bcc-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-4f073bcc-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-4f073bcc-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-4f073bcc-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44975
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39859
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44975', name: tcp://127.0.0.1:44975, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44975
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39859', name: tcp://127.0.0.1:39859, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39859
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_estimators_fit_returns_self(readonly_memmap=True)] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_fit_returns_self at 0x7f87759d7040>, 'DaskLGBMClassifier', readonly_memmap=True)
client = <Client: 'tcp://127.0.0.1:38527' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2058: in check_estimators_fit_returns_self
    assert estimator.fit(X, y) is estimator
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = memmap([[5.44338915, 4.50900038],
        [4.52251198, 6.68286579],
        [5.33420024, 3.94365401],
        [6.15287...98, 2.1500414 ],
        [0.45347483, 6.92854682],
        [4.97048201, 7.65863655],
        [2.93656087, 7.35343631]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'memmap' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:38527
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45355
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45355
distributed.worker - INFO -          dashboard at:            127.0.0.1:33741
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38527
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-ca88209a-a46d-46e7-9226-03523db17a34/dask-worker-space/worker-iq8xy74l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43849
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43849
distributed.worker - INFO -          dashboard at:            127.0.0.1:41595
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38527
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-38ec0334-717c-4175-ad7a-9d4dcd0371be/dask-worker-space/worker-g1dbwsln
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45355', name: tcp://127.0.0.1:45355, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45355
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38527
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:43849', name: tcp://127.0.0.1:43849, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43849
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38527
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-5167a364-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-5167a364-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-5167a364-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-5167a364-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45355
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43849
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45355', name: tcp://127.0.0.1:45355, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45355
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:43849', name: tcp://127.0.0.1:43849, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:43849
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMClassifier()-check_complex_data] _______

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_complex_data at 0x7f87759cee50>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:40053' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:937: in check_complex_data
    assert_raises_regex(ValueError, "Complex data not supported",
/opt/conda/lib/python3.8/unittest/case.py:1357: in assertRaisesRegex
    return context.handle('assertRaisesRegex', args, kwargs)
/opt/conda/lib/python3.8/unittest/case.py:202: in handle
    callable_obj(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:40053
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42969
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42969
distributed.worker - INFO -          dashboard at:            127.0.0.1:44199
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40053
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-27b9fc37-965f-494c-80f6-3b0bfb6711fa/dask-worker-space/worker-gw42ejlv
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42969', name: tcp://127.0.0.1:42969, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42969
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40053
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37901
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37901
distributed.worker - INFO -          dashboard at:            127.0.0.1:36453
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40053
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-9d076983-4ad1-4d3a-b52d-735412807cbb/dask-worker-space/worker-48k4hjfz
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37901', name: tcp://127.0.0.1:37901, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37901
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40053
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-53b25148-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-53b25148-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-53b25148-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-53b25148-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37901
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42969
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42969', name: tcp://127.0.0.1:42969, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42969
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37901', name: tcp://127.0.0.1:37901, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37901
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMClassifier()-check_dtype_object] _______

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_dtype_object at 0x7f87759cedc0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:43313' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:906: in check_dtype_object
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135039273248, 0.7151893663724195, 0.6027633760716439,
        0.5448831829968969, 0.4236547993389047, 0....4736,
        0.3553688484719296, 0.3567068904025429, 0.01632850268370789,
        0.18523232523618394]], dtype=object)
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43313
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39591
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39591
distributed.worker - INFO -          dashboard at:            127.0.0.1:39665
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43313
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-96ac1536-a4e1-49fc-bf6a-404ca672e4e3/dask-worker-space/worker-s6v_8l1q
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39591', name: tcp://127.0.0.1:39591, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39591
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43313
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41289
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41289
distributed.worker - INFO -          dashboard at:            127.0.0.1:40853
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43313
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-68440177-7e9e-4214-b576-370364ec8c28/dask-worker-space/worker-5nmbhf11
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:41289', name: tcp://127.0.0.1:41289, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41289
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43313
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-5620c103-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-5620c103-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-5620c103-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-5620c103-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39591
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41289
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:41289', name: tcp://127.0.0.1:41289, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:41289
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39591', name: tcp://127.0.0.1:39591, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39591
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_estimators_empty_data_messages] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_empty_data_messages at 0x7f87759d4160>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:44447' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1427: in check_estimators_empty_data_messages
    e.fit(X_zero_samples, [])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:44447
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33193
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33193
distributed.worker - INFO -          dashboard at:            127.0.0.1:46337
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-1075992b-e5e3-40a8-abaf-f30ccd52e97d/dask-worker-space/worker-5bw4w9la
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:33193', name: tcp://127.0.0.1:33193, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33193
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38659
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38659
distributed.worker - INFO -          dashboard at:            127.0.0.1:41731
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-c5b105d6-15b6-43e8-8bbc-d17bec22901f/dask-worker-space/worker-7s4cblw0
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38659', name: tcp://127.0.0.1:38659, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38659
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44447
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-585ab689-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-585ab689-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-585ab689-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-585ab689-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33193
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38659
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:33193', name: tcp://127.0.0.1:33193, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:33193
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38659', name: tcp://127.0.0.1:38659, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38659
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
__ test_sklearn_integration[DaskLGBMClassifier()-check_pipeline_consistency] ___

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_pipeline_consistency at 0x7f87759d0dc0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:45211' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1349: in check_pipeline_consistency
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.26049352, 1.32820804, 1.26819727],
       [0.37832805, 0.37553697, 0.2165663 ],
       [0.29635883, 0.269703...5 , 1.14822372, 1.36074415],
       [1.18681797, 1.16821927, 1.19741402],
       [0.22506871, 0.15044369, 0.11329719]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:45211
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36547
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36547
distributed.worker - INFO -          dashboard at:            127.0.0.1:35199
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45211
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-14a65b06-70f7-4e21-97d9-7eebe30f97d9/dask-worker-space/worker-2a6w48b1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36547', name: tcp://127.0.0.1:36547, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45211
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36547
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43357
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43357
distributed.worker - INFO -          dashboard at:            127.0.0.1:42235
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45211
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-56a2886f-50a7-4aa6-b6e1-7fba80adf9ac/dask-worker-space/worker-z3qck7np
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:43357', name: tcp://127.0.0.1:43357, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43357
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45211
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-5ac419eb-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-5ac419eb-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-5ac419eb-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-5ac419eb-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36547
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43357
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36547', name: tcp://127.0.0.1:36547, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36547
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:43357', name: tcp://127.0.0.1:43357, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:43357
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_estimators_overwrite_params] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_overwrite_params at 0x7f87759d7dc0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:34579' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2427: in check_estimators_overwrite_params
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[5.44338915, 4.50900038],
       [4.52251198, 6.68286579],
       [5.33420024, 3.94365401],
       [6.15287792,...65598, 2.1500414 ],
       [0.45347483, 6.92854682],
       [4.97048201, 7.65863655],
       [2.93656087, 7.35343631]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:34579
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44065
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44065
distributed.worker - INFO -          dashboard at:            127.0.0.1:45841
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-19741d5a-b5eb-4eff-ad70-2d83e849b819/dask-worker-space/worker-60ckm2yq
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44065', name: tcp://127.0.0.1:44065, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44065
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42357
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42357
distributed.worker - INFO -          dashboard at:            127.0.0.1:42581
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-44f5cb2a-73b0-4f86-8bff-1e6022136b53/dask-worker-space/worker-j1d0kcxb
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42357', name: tcp://127.0.0.1:42357, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42357
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-5d66b200-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-5d66b200-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-5d66b200-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-5d66b200-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42357
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44065
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44065', name: tcp://127.0.0.1:44065, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44065
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42357', name: tcp://127.0.0.1:42357, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42357
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
__ test_sklearn_integration[DaskLGBMClassifier()-check_estimator_sparse_data] __

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimator_sparse_data at 0x7f87759ce700>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:35281' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:737: in check_estimator_sparse_data
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: in _split_to_parts
    parts = data.to_delayed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <40x10 sparse matrix of type '<class 'numpy.float64'>'
	with 74 stored elements in Compressed Sparse Row format>
attr = 'to_delayed'

    def __getattr__(self, attr):
        if attr == 'A':
            return self.toarray()
        elif attr == 'T':
            return self.transpose()
        elif attr == 'H':
            return self.getH()
        elif attr == 'real':
            return self._real()
        elif attr == 'imag':
            return self._imag()
        elif attr == 'size':
            return self.getnnz()
        else:
>           raise AttributeError(attr + " not found")
E           AttributeError: to_delayed not found

/opt/conda/lib/python3.8/site-packages/scipy/sparse/base.py:687: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:35281
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42313
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42313
distributed.worker - INFO -          dashboard at:            127.0.0.1:33571
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35281
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-565b29aa-cc12-41f0-8a8a-3b60c9ac8b6b/dask-worker-space/worker-_10odo62
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46461
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46461
distributed.worker - INFO -          dashboard at:            127.0.0.1:40393
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35281
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-dd878690-19aa-4804-8a45-0c09cabc88d6/dask-worker-space/worker-43ccsucl
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42313', name: tcp://127.0.0.1:42313, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42313
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:46461', name: tcp://127.0.0.1:46461, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46461
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35281
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35281
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-605c4715-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
----------------------------- Captured stdout call -----------------------------
Estimator DaskLGBMClassifier doesn't seem to fail gracefully on sparse data: it should raise a TypeError if sparse input is explicitly not supported.
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-605c4715-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-605c4715-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-605c4715-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46461
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42313
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:46461', name: tcp://127.0.0.1:46461, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:46461
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42313', name: tcp://127.0.0.1:42313, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42313
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
____ test_sklearn_integration[DaskLGBMClassifier()-check_estimators_pickle] ____

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_pickle at 0x7f87759d44c0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:37871' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1548: in check_estimators_pickle
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[       nan, 1.32820804, 1.26819727],
       [0.37832805, 0.37553697, 0.2165663 ],
       [0.29635883, 0.269703...5 , 1.14822372, 1.36074415],
       [1.18681797, 1.16821927, 1.19741402],
       [0.22506871, 0.15044369, 0.11329719]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:37871
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35743
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35743
distributed.worker - INFO -          dashboard at:            127.0.0.1:39709
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37871
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-a8ea15b7-7e96-457a-a2d3-11c15109e605/dask-worker-space/worker-iz224rin
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35743', name: tcp://127.0.0.1:35743, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35743
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37871
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46209
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46209
distributed.worker - INFO -          dashboard at:            127.0.0.1:43839
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37871
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-bc5c7235-3b9b-4212-bdbc-ffebf7f16ea4/dask-worker-space/worker-gz_cvetb
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:46209', name: tcp://127.0.0.1:46209, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37871
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46209
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-6300e00d-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-6300e00d-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-6300e00d-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-6300e00d-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35743
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46209
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35743', name: tcp://127.0.0.1:35743, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35743
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:46209', name: tcp://127.0.0.1:46209, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:46209
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_classifier_data_not_an_array] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_classifier_data_not_an_array at 0x7f87759db160>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:38963' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2512: in check_classifier_data_not_an_array
    check_estimators_data_not_an_array(name, estimator_orig, X, y,
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2562: in check_estimators_data_not_an_array
    estimator_1.fit(X_, y_)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = <sklearn.utils.estimator_checks._NotAnArray object at 0x7f8767f9b790>
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: '_NotAnArray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:38963
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42253
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42253
distributed.worker - INFO -          dashboard at:            127.0.0.1:41109
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38963
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-dab2a251-434c-4e24-ab0d-942c359e7fbd/dask-worker-space/worker-sb4p_v6o
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42253', name: tcp://127.0.0.1:42253, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42253
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38963
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37765
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37765
distributed.worker - INFO -          dashboard at:            127.0.0.1:35655
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38963
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-a01a74d0-3490-40c6-9a7b-f6465fbbdf7a/dask-worker-space/worker-c2y5wxd8
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37765', name: tcp://127.0.0.1:37765, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37765
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38963
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-659a2ce3-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-659a2ce3-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-659a2ce3-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-659a2ce3-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37765
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42253
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42253', name: tcp://127.0.0.1:42253, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42253
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37765', name: tcp://127.0.0.1:37765, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37765
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
__ test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_one_label] __

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_classifiers_one_label at 0x7f87759d4b80>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:42757' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1778: in check_classifiers_one_label
    raise exc
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1767: in check_classifiers_one_label
    classifier.fit(X_train, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135 , 0.71518937, 0.60276338],
       [0.54488318, 0.4236548 , 0.64589411],
       [0.43758721, 0.891773...56, 0.46147936, 0.78052918],
       [0.11827443, 0.63992102, 0.14335329],
       [0.94466892, 0.52184832, 0.41466194]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:42757
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35381
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35381
distributed.worker - INFO -          dashboard at:            127.0.0.1:43557
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42757
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-d94f715e-f9ef-4088-8cf9-0d5e19ca48b2/dask-worker-space/worker-wezz2ag2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44519
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44519
distributed.worker - INFO -          dashboard at:            127.0.0.1:46169
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42757
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-48be4ced-717f-41ce-9baa-831d0f9bff36/dask-worker-space/worker-glj9x9at
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44519', name: tcp://127.0.0.1:44519, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44519
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42757
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35381', name: tcp://127.0.0.1:35381, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35381
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42757
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-67f14fc0-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
----------------------------- Captured stdout call -----------------------------
Classifier can't train when only one class is present. DaskLGBMClassifier(local_listen_port=18000, time_out=5) 'numpy.ndarray' object has no attribute 'to_delayed'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py", line 1767, in check_classifiers_one_label
    classifier.fit(X_train, y)
  File "/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py", line 609, in fit
    return self._fit(
  File "/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py", line 498, in _fit
    model = _train(
  File "/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py", line 288, in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
  File "/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py", line 199, in _split_to_parts
    parts = data.to_delayed()
AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-67f14fc0-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-67f14fc0-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-67f14fc0-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35381
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44519
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44519', name: tcp://127.0.0.1:44519, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44519
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35381', name: tcp://127.0.0.1:35381, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35381
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_classes] ___

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_classifiers_classes at 0x7f87759d75e0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:36343' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2198: in check_classifiers_classes
    check_classifiers_predictions(X, y_, name, classifier_orig)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2122: in check_classifiers_predictions
    classifier.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.86111069, 2.78745871],
       [1.94936262, 2.66852288],
       [2.0000686 , 2.57898891],
       [2.48849206,...70993, 2.70714243],
       [2.534293  , 0.25317482],
       [2.56716497, 0.26907278],
       [1.91380191, 2.67237895]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:36343
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37385
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37385
distributed.worker - INFO -          dashboard at:            127.0.0.1:34285
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36343
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-92542f92-bb7b-481c-8a17-27f7302b74e7/dask-worker-space/worker-n4p_b_9q
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37385', name: tcp://127.0.0.1:37385, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37385
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36343
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45933
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45933
distributed.worker - INFO -          dashboard at:            127.0.0.1:37521
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36343
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-5a83dca4-6969-41c7-8ea2-46e2af5bbf39/dask-worker-space/worker-6r39zxlp
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45933', name: tcp://127.0.0.1:45933, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45933
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36343
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-6a48ab43-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-6a48ab43-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-6a48ab43-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-6a48ab43-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45933
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37385
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37385', name: tcp://127.0.0.1:37385, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37385
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45933', name: tcp://127.0.0.1:45933, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45933
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
____ test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_train] ____

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_classifiers_train at 0x7f87759d4ca0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:39689' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1829: in check_classifiers_train
    classifier.fit(X, y[:-1])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:39689
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35117
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35117
distributed.worker - INFO -          dashboard at:            127.0.0.1:39179
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39689
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-fd6bcd77-07b4-4888-b9e1-a27a6be65949/dask-worker-space/worker-k_ef08rl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45029
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45029
distributed.worker - INFO -          dashboard at:            127.0.0.1:34855
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39689
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35117', name: tcp://127.0.0.1:35117, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35117
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39689
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-2eaf6040-afd5-41e2-bf08-5d8714c8200c/dask-worker-space/worker-5tp7tn6s
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45029', name: tcp://127.0.0.1:45029, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45029
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39689
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-6ecd5a7c-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-6ecd5a7c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-6ecd5a7c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-6ecd5a7c-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35117
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45029
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35117', name: tcp://127.0.0.1:35117, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35117
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45029', name: tcp://127.0.0.1:45029, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45029
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_train(readonly_memmap=True)] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_classifiers_train at 0x7f87759d4ca0>, 'DaskLGBMClassifier', readonly_memmap=True)
client = <Client: 'tcp://127.0.0.1:42751' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1829: in check_classifiers_train
    classifier.fit(X, y[:-1])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'memmap' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:42751
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45693
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45693
distributed.worker - INFO -          dashboard at:            127.0.0.1:34097
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42751
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-c2d66891-34ad-4190-a462-447df11b4c4a/dask-worker-space/worker-pprkq_7i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35613
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35613
distributed.worker - INFO -          dashboard at:            127.0.0.1:41441
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42751
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-84841025-2e21-4385-a66a-2be26eba97b1/dask-worker-space/worker-irt4wsta
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45693', name: tcp://127.0.0.1:45693, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45693
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35613', name: tcp://127.0.0.1:35613, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35613
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42751
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42751
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-711f9777-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-711f9777-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-711f9777-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-711f9777-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35613
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45693
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35613', name: tcp://127.0.0.1:35613, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35613
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45693', name: tcp://127.0.0.1:45693, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45693
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_train(readonly_memmap=True,X_dtype=float32)] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_classifiers_train at 0x7f87759d4ca0>, 'DaskLGBMClassifier', readonly_memmap=True, X_dtype='float32')
client = <Client: 'tcp://127.0.0.1:43687' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1829: in check_classifiers_train
    classifier.fit(X, y[:-1])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'memmap' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43687
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45975
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45975
distributed.worker - INFO -          dashboard at:            127.0.0.1:33305
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43687
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-99c5231a-9b86-436c-9c34-c6d268b1db1a/dask-worker-space/worker-eino1cwd
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45975', name: tcp://127.0.0.1:45975, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45975
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43687
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41099
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41099
distributed.worker - INFO -          dashboard at:            127.0.0.1:35797
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43687
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-6207a31e-78f8-46d9-bd54-60ffd1fe09c8/dask-worker-space/worker-_49uu8wq
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:41099', name: tcp://127.0.0.1:41099, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41099
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43687
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7380b3b1-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-7380b3b1-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-7380b3b1-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-7380b3b1-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45975
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41099
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45975', name: tcp://127.0.0.1:45975, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45975
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:41099', name: tcp://127.0.0.1:41099, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:41099
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_regression_target] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_classifiers_regression_target at 0x7f87759dbc10>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:36043' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2816: in check_classifiers_regression_target
    assert_raises_regex(ValueError, msg, e.fit, X, y)
/opt/conda/lib/python3.8/unittest/case.py:1357: in assertRaisesRegex
    return context.handle('assertRaisesRegex', args, kwargs)
/opt/conda/lib/python3.8/unittest/case.py:202: in handle
    callable_obj(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:36043
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36567
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36567
distributed.worker - INFO -          dashboard at:            127.0.0.1:45273
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36043
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-4629fae8-6ae5-486a-a111-6f8899339820/dask-worker-space/worker-gcvrq4mb
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36567', name: tcp://127.0.0.1:36567, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36567
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36043
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46297
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46297
distributed.worker - INFO -          dashboard at:            127.0.0.1:43777
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36043
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-44f41ed9-a61b-4105-83af-38a763c7edaa/dask-worker-space/worker-3q4sqthx
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:46297', name: tcp://127.0.0.1:46297, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46297
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36043
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-75e1c035-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-75e1c035-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-75e1c035-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-75e1c035-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46297
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36567
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:46297', name: tcp://127.0.0.1:46297, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:46297
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36567', name: tcp://127.0.0.1:36567, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36567
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMClassifier()-check_supervised_y_no_nan] ___

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_supervised_y_no_nan at 0x7f87759cd790>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:39601' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:149: in check_supervised_y_no_nan
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[-1.76200874e-01,  1.88876361e-01,  8.26747180e-01,
        -3.24473146e-02, -6.52499418e-01],
       [-1.05339...1.06098418e+00],
       [ 1.31845313e-01,  1.71934297e-01,  8.88062794e-02,
         1.03963016e+00, -1.37846942e-01]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:39601
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33747
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33747
distributed.worker - INFO -          dashboard at:            127.0.0.1:43853
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39601
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-a617953f-a091-4048-abd7-cf70866934e7/dask-worker-space/worker-5pxqgsrq
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:33747', name: tcp://127.0.0.1:33747, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33747
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39601
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33083
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33083
distributed.worker - INFO -          dashboard at:            127.0.0.1:36825
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39601
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-3d6b3c53-b514-4415-b22e-7b1159dbacab/dask-worker-space/worker-_ur7bk93
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:33083', name: tcp://127.0.0.1:33083, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33083
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39601
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-783cd134-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-783cd134-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-783cd134-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-783cd134-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33083
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33747
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:33083', name: tcp://127.0.0.1:33083, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:33083
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:33747', name: tcp://127.0.0.1:33747, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:33747
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMClassifier()-check_supervised_y_2d] _____

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_supervised_y_2d at 0x7f87759d7280>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:38845' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2093: in check_supervised_y_2d
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135 , 0.71518937, 0.60276338],
       [0.54488318, 0.4236548 , 0.64589411],
       [0.43758721, 0.891773...99, 0.0641475 , 0.69247212],
       [0.56660145, 0.26538949, 0.52324805],
       [0.09394051, 0.5759465 , 0.9292962 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:38845
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38627
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38627
distributed.worker - INFO -          dashboard at:            127.0.0.1:44573
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-1f1b285c-fd98-4d45-82b8-96204cfa11fb/dask-worker-space/worker-ip51vlly
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38627', name: tcp://127.0.0.1:38627, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38627
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38845
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44487
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44487
distributed.worker - INFO -          dashboard at:            127.0.0.1:35229
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38845
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-8df685bd-78a9-47ff-b4f7-10b4fd351a3d/dask-worker-space/worker-tfkg8uth
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44487', name: tcp://127.0.0.1:44487, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44487
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38845
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7a790508-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-7a790508-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-7a790508-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-7a790508-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38627
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44487
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38627', name: tcp://127.0.0.1:38627, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38627
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44487', name: tcp://127.0.0.1:44487, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44487
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMClassifier()-check_estimators_unfitted] ___

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_unfitted at 0x7f87759d7160>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:37627' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2074: in check_estimators_unfitted
    assert_raises(NotFittedError, getattr(estimator, method), X)
/opt/conda/lib/python3.8/unittest/case.py:816: in assertRaises
    return context.handle('assertRaises', args, kwargs)
/opt/conda/lib/python3.8/unittest/case.py:202: in handle
    callable_obj(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:655: in predict_proba
    return _predict(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _predict(
        model: LGBMModel,
        data: _DaskMatrixLike,
        raw_score: bool = False,
        pred_proba: bool = False,
        pred_leaf: bool = False,
        pred_contrib: bool = False,
        dtype: _PredictionDtype = np.float32,
        **kwargs: Any
    ) -> dask_Array:
        """Inner predict routine.
    
        Parameters
        ----------
        model : lightgbm.LGBMClassifier, lightgbm.LGBMRegressor, or lightgbm.LGBMRanker class
            Fitted underlying model.
        data : Dask Array or Dask DataFrame of shape = [n_samples, n_features]
            Input feature matrix.
        raw_score : bool, optional (default=False)
            Whether to predict raw scores.
        pred_proba : bool, optional (default=False)
            Should method return results of ``predict_proba`` (``pred_proba=True``) or ``predict`` (``pred_proba=False``).
        pred_leaf : bool, optional (default=False)
            Whether to predict leaf index.
        pred_contrib : bool, optional (default=False)
            Whether to predict feature contributions.
        dtype : np.dtype, optional (default=np.float32)
            Dtype of the output.
        **kwargs
            Other parameters passed to ``predict`` or ``predict_proba`` method.
    
        Returns
        -------
        predicted_result : Dask Array of shape = [n_samples] or shape = [n_samples, n_classes]
            The predicted values.
        X_leaves : Dask Array of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]
            If ``pred_leaf=True``, the predicted leaf of every tree for each sample.
        X_SHAP_values : Dask Array of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes]
            If ``pred_contrib=True``, the feature contributions for each sample.
        """
        if not all((DASK_INSTALLED, PANDAS_INSTALLED, SKLEARN_INSTALLED)):
            raise LightGBMError('dask, pandas and scikit-learn are required for lightgbm.dask')
        if isinstance(data, dask_DataFrame):
            return data.map_partitions(
                _predict_part,
                model=model,
                raw_score=raw_score,
                pred_proba=pred_proba,
                pred_leaf=pred_leaf,
                pred_contrib=pred_contrib,
                **kwargs
            ).values
        elif isinstance(data, dask_Array):
            if pred_proba:
                kwargs['chunks'] = (data.chunks[0], (model.n_classes_,))
            else:
                kwargs['drop_axis'] = 1
            return data.map_blocks(
                _predict_part,
                model=model,
                raw_score=raw_score,
                pred_proba=pred_proba,
                pred_leaf=pred_leaf,
                pred_contrib=pred_contrib,
                dtype=dtype,
                **kwargs
            )
        else:
>           raise TypeError('Data must be either Dask Array or Dask DataFrame. Got %s.' % str(type(data)))
E           TypeError: Data must be either Dask Array or Dask DataFrame. Got <class 'numpy.ndarray'>.

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:457: TypeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:37627
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33781
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33781
distributed.worker - INFO -          dashboard at:            127.0.0.1:34537
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37627
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-0cf42cc0-c35a-42b7-bf09-8cdfb133cd75/dask-worker-space/worker-883jakxp
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:33781', name: tcp://127.0.0.1:33781, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33781
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37627
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41355
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41355
distributed.worker - INFO -          dashboard at:            127.0.0.1:39157
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37627
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-0748af1e-e4e7-4395-a188-56d391fa4d7e/dask-worker-space/worker-iam_ke8r
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:41355', name: tcp://127.0.0.1:41355, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41355
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37627
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7d0dcc22-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-7d0dcc22-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-7d0dcc22-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-7d0dcc22-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33781
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41355
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:41355', name: tcp://127.0.0.1:41355, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:41355
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:33781', name: tcp://127.0.0.1:33781, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:33781
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_class_weight_classifiers] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_class_weight_classifiers at 0x7f87759d7a60>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:44039' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2345: in check_class_weight_classifiers
    classifier.fit(X_train, y_train)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[ 19.19187976, -12.12284821],
       [ 14.55989654, -31.14348945],
       [ 45.31998651,  27.62822265],
       ...77027],
       [  3.8571415 ,  33.38925747],
       [ -5.0246107 , -26.60136221],
       [-21.40720058,  39.77008737]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:44039
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46369
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46369
distributed.worker - INFO -          dashboard at:            127.0.0.1:35637
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44039
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-248c24b0-7bfe-431c-a67b-1e1b716cd174/dask-worker-space/worker-w_8xlv44
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:46369', name: tcp://127.0.0.1:46369, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44039
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46369
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42077
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42077
distributed.worker - INFO -          dashboard at:            127.0.0.1:42875
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44039
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-8686b4c3-8da0-42e6-a0be-945396983d0d/dask-worker-space/worker-rcugt6h2
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42077', name: tcp://127.0.0.1:42077, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42077
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44039
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7f8b41cf-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-7f8b41cf-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-7f8b41cf-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-7f8b41cf-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42077
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46369
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:46369', name: tcp://127.0.0.1:46369, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:46369
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42077', name: tcp://127.0.0.1:42077, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42077
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMClassifier()-check_fit2d_predict1d] _____

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit2d_predict1d at 0x7f87759d03a0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:35719' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1061: in check_fit2d_predict1d
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013],
       [1.63464955, 1.2709644 , 1.93768234],
       [1.31276163, 2.675319...54, 2.96512151, 0.30613443],
       [0.62663027, 0.48392855, 1.95932498],
       [0.75987481, 1.39893232, 0.73327678]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:35719
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42281
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42281
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34639
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34639
distributed.worker - INFO -          dashboard at:            127.0.0.1:46513
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35719
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-944cca2c-2adc-4ecb-bb22-525bfe3746e8/dask-worker-space/worker-ji0ppuwy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:            127.0.0.1:36117
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35719
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-2e0e0d5d-379f-4e42-8ace-ca68b6c350b3/dask-worker-space/worker-6ma9o3n1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42281', name: tcp://127.0.0.1:42281, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42281
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35719
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34639', name: tcp://127.0.0.1:34639, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34639
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35719
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-863bcb2c-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-863bcb2c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-863bcb2c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-863bcb2c-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42281
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42281', name: tcp://127.0.0.1:42281, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42281
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34639
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34639', name: tcp://127.0.0.1:34639, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34639
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_methods_subset_invariance] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_methods_subset_invariance at 0x7f87759d0550>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:39131' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1109: in check_methods_subset_invariance
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013],
       [1.63464955, 1.2709644 , 1.93768234],
       [1.31276163, 2.675319...54, 2.96512151, 0.30613443],
       [0.62663027, 0.48392855, 1.95932498],
       [0.75987481, 1.39893232, 0.73327678]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:39131
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45403
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45403
distributed.worker - INFO -          dashboard at:            127.0.0.1:35563
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39131
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-b460e32a-bcf0-4ad8-9005-79d46a07f06b/dask-worker-space/worker-63hhdfi1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45403', name: tcp://127.0.0.1:45403, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45403
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39131
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39863
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39863
distributed.worker - INFO -          dashboard at:            127.0.0.1:39603
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39131
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-4e34deae-36b7-46a0-97c5-2b37e1914016/dask-worker-space/worker-u7mwhdy4
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39863', name: tcp://127.0.0.1:39863, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39863
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39131
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-8885ada8-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-8885ada8-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-8885ada8-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-8885ada8-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39863
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45403
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45403', name: tcp://127.0.0.1:45403, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45403
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39863', name: tcp://127.0.0.1:39863, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39863
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMClassifier()-check_fit2d_1sample] ______

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit2d_1sample at 0x7f87759d0670>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:38097' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1152: in check_fit2d_1sample
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013, 1.63464955, 1.2709644 ,
        1.93768234, 1.31276163, 2.675319  , 2.89098828, 1.15032456]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:38097
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35341
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35341
distributed.worker - INFO -          dashboard at:            127.0.0.1:39775
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38097
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-61772a3f-5721-4a77-9ed0-33e45686835b/dask-worker-space/worker-8qujycr_
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35341', name: tcp://127.0.0.1:35341, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35341
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38097
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38989
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38989
distributed.worker - INFO -          dashboard at:            127.0.0.1:38085
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38097
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-fa2cfe20-52c0-4915-891e-cce6c4928293/dask-worker-space/worker-drdl027g
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38989', name: tcp://127.0.0.1:38989, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38989
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38097
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-8addef73-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-8addef73-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-8addef73-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-8addef73-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38989
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35341
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38989', name: tcp://127.0.0.1:38989, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38989
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35341', name: tcp://127.0.0.1:35341, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35341
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMClassifier()-check_fit2d_1feature] ______

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit2d_1feature at 0x7f87759d0790>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:44643' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1186: in check_fit2d_1feature
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051],
       [2.1455681 ],
       [1.80829013],
       [1.63464955],
       [1.2709644 ],
       [1.93768234],
       [1.31276163],
       [2.675319  ],
       [2.89098828],
       [1.15032456]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:44643
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36115
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36115
distributed.worker - INFO -          dashboard at:            127.0.0.1:40203
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44643
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-0ad72b67-a7c0-4989-a898-4a38523ff533/dask-worker-space/worker-pg4w0hff
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34835
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34835
distributed.worker - INFO -          dashboard at:            127.0.0.1:44937
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44643
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-6dc8cd93-c311-4535-b447-65366603f89a/dask-worker-space/worker-sbhr7kna
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36115', name: tcp://127.0.0.1:36115, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36115
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44643
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34835', name: tcp://127.0.0.1:34835, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34835
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44643
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-8d3c1697-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-8d3c1697-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-8d3c1697-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-8d3c1697-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36115
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34835
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34835', name: tcp://127.0.0.1:34835, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34835
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36115', name: tcp://127.0.0.1:36115, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36115
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
__________ test_sklearn_integration[DaskLGBMClassifier()-check_fit1d] __________

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit1d at 0x7f87759d08b0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:40679' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1211: in check_fit1d
    assert_raises(ValueError, estimator.fit, X, y)
/opt/conda/lib/python3.8/unittest/case.py:816: in assertRaises
    return context.handle('assertRaises', args, kwargs)
/opt/conda/lib/python3.8/unittest/case.py:202: in handle
    callable_obj(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:40679
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44437
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44437
distributed.worker - INFO -          dashboard at:            127.0.0.1:45801
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40679
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-4227b1b3-c86f-4ac9-9b54-4e12d5d56887/dask-worker-space/worker-2rkkizq4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40225
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40225
distributed.worker - INFO -          dashboard at:            127.0.0.1:41893
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40679
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-e4c911f5-5b3b-4810-887c-1ef185de9f82/dask-worker-space/worker-g9mw1zb5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44437', name: tcp://127.0.0.1:44437, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44437
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40679
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40225', name: tcp://127.0.0.1:40225, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40225
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40679
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-8f8a6510-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-8f8a6510-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-8f8a6510-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-8f8a6510-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44437
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40225
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44437', name: tcp://127.0.0.1:44437, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44437
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40225', name: tcp://127.0.0.1:40225, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40225
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMClassifier()-check_dict_unchanged] ______

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_dict_unchanged at 0x7f87759cef70>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:46527' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:971: in check_dict_unchanged
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.09762701, 1.43037873, 1.20552675],
       [1.08976637, 0.8473096 , 1.29178823],
       [0.87517442, 1.783546...03, 1.97674768, 0.20408962],
       [0.41775351, 0.32261904, 1.30621665],
       [0.50658321, 0.93262155, 0.48885118]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:46527
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35145
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35145
distributed.worker - INFO -          dashboard at:            127.0.0.1:46525
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46527
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-5a403014-7fcb-48c5-86e1-7f43b274864e/dask-worker-space/worker-cjukbs66
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35145', name: tcp://127.0.0.1:35145, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35145
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46527
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32899
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32899
distributed.worker - INFO -          dashboard at:            127.0.0.1:44213
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46527
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-a65262cd-d232-4d87-9cda-4c95ad29a9ad/dask-worker-space/worker-932u5ycq
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:32899', name: tcp://127.0.0.1:32899, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32899
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46527
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-96399607-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-96399607-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-96399607-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-96399607-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35145
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32899
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:32899', name: tcp://127.0.0.1:32899, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:32899
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35145', name: tcp://127.0.0.1:35145, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35145
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMClassifier()-check_dont_overwrite_parameters] _

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_dont_overwrite_parameters at 0x7f87759d0280>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:35683' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1011: in check_dont_overwrite_parameters
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013],
       [1.63464955, 1.2709644 , 1.93768234],
       [1.31276163, 2.675319...54, 2.96512151, 0.30613443],
       [0.62663027, 0.48392855, 1.95932498],
       [0.75987481, 1.39893232, 0.73327678]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:35683
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36233
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36233
distributed.worker - INFO -          dashboard at:            127.0.0.1:39385
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35683
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-b68926a5-8247-46eb-a10e-2f581712057b/dask-worker-space/worker-drt3by5w
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36233', name: tcp://127.0.0.1:36233, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36233
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35683
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34255
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34255
distributed.worker - INFO -          dashboard at:            127.0.0.1:42575
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35683
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-9d98e0ff-28e4-470d-af86-40ab2e302d75/dask-worker-space/worker-y0sgamlz
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34255', name: tcp://127.0.0.1:34255, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34255
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35683
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-987c431a-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-987c431a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-987c431a-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36233
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34255
distributed.scheduler - INFO - Close client connection: Client-987c431a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36233', name: tcp://127.0.0.1:36233, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36233
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34255', name: tcp://127.0.0.1:34255, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34255
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMClassifier()-check_fit_idempotent] ______

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit_idempotent at 0x7f87759dbee0>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:43081' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2930: in check_fit_idempotent
    estimator.fit(X_train, y_train)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[ 99.30543214,  99.85036546],
       [ 98.82687659, 101.94362119],
       [ 99.260437  , 101.5430146 ],
       ...07516],
       [100.52327666,  99.82845367],
       [100.77179055, 100.82350415],
       [ 98.68409259,  99.5384154 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43081
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37587
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37587
distributed.worker - INFO -          dashboard at:            127.0.0.1:35329
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43081
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-55529ce7-7021-4c45-bb25-2193f1f4fe90/dask-worker-space/worker-zg5dt8yw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45005
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45005
distributed.worker - INFO -          dashboard at:            127.0.0.1:34863
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43081
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-91377708-0f48-4023-923f-84190e80ad46/dask-worker-space/worker-ffno_jy4
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37587', name: tcp://127.0.0.1:37587, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37587
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43081
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45005', name: tcp://127.0.0.1:45005, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45005
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43081
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-9acd5ede-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-9acd5ede-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-9acd5ede-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-9acd5ede-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37587
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45005
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37587', name: tcp://127.0.0.1:37587, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37587
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45005', name: tcp://127.0.0.1:45005, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45005
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMClassifier()-check_n_features_in] ______

estimator = DaskLGBMClassifier(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_n_features_in at 0x7f87759dbf70>, 'DaskLGBMClassifier')
client = <Client: 'tcp://127.0.0.1:34597' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2975: in check_n_features_in
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:609: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[101.76405235, 100.40015721],
       [100.97873798, 102.2408932 ],
       [101.86755799,  99.02272212],
       ...9065 ],
       [100.52327666,  99.82845367],
       [100.77179055, 100.82350415],
       [102.16323595, 101.33652795]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:34597
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34517
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34517
distributed.worker - INFO -          dashboard at:            127.0.0.1:44137
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34597
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-da0468d2-6713-4ee0-ad3b-a6f1b46a014e/dask-worker-space/worker-c7llym8m
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34517', name: tcp://127.0.0.1:34517, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34517
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34597
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34283
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34283
distributed.worker - INFO -          dashboard at:            127.0.0.1:35391
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34597
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-f674aeb1-84c5-47c8-bca4-d2894078c4ec/dask-worker-space/worker-yjgkczrv
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34283', name: tcp://127.0.0.1:34283, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34283
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34597
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-9d22d1dc-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-9d22d1dc-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-9d22d1dc-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-9d22d1dc-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34283
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34517
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34517', name: tcp://127.0.0.1:34517, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34517
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34283', name: tcp://127.0.0.1:34283, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34283
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
____ test_sklearn_integration[DaskLGBMRegressor()-check_estimators_dtypes] _____

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_dtypes at 0x7f87759d4040>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:40793' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1408: in check_estimators_dtypes
    estimator.fit(X_train, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.6464405 , 2.145568  , 1.80829   , 1.6346495 , 1.2709644 ],
       [1.9376824 , 1.3127615 , 2.675319  , 2.890...6 , 2.1489816 , 0.8682183 ],
       [0.5495741 , 1.7595388 , 0.06032264, 2.4868202 , 0.01408643]],
      dtype=float32)
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:40793
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34025
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34025
distributed.worker - INFO -          dashboard at:            127.0.0.1:35077
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40793
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-2b73fcf9-14e6-4804-89c0-3a491e4207d5/dask-worker-space/worker-b9yaqz4h
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34025', name: tcp://127.0.0.1:34025, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34025
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40793
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36909
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36909
distributed.worker - INFO -          dashboard at:            127.0.0.1:43217
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40793
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-ebb554c1-18ae-4f14-9c5f-99ba78120ac7/dask-worker-space/worker-ez16ge_v
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36909', name: tcp://127.0.0.1:36909, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40793
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36909
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-a1a8e1c1-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-a1a8e1c1-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-a1a8e1c1-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-a1a8e1c1-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34025
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36909
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34025', name: tcp://127.0.0.1:34025, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34025
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36909', name: tcp://127.0.0.1:36909, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36909
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
____ test_sklearn_integration[DaskLGBMRegressor()-check_fit_score_takes_y] _____

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit_score_takes_y at 0x7f87759d0ee0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:37233' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1380: in check_fit_score_takes_y
    func(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135 , 0.71518937, 0.60276338],
       [0.54488318, 0.4236548 , 0.64589411],
       [0.43758721, 0.891773...99, 0.0641475 , 0.69247212],
       [0.56660145, 0.26538949, 0.52324805],
       [0.09394051, 0.5759465 , 0.9292962 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:37233
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38225
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38225
distributed.worker - INFO -          dashboard at:            127.0.0.1:38449
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37233
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-ad598935-e084-4e54-9053-1ca884feca57/dask-worker-space/worker-qp4o5p_1
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38225', name: tcp://127.0.0.1:38225, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38225
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37233
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38319
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38319
distributed.worker - INFO -          dashboard at:            127.0.0.1:42633
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37233
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-d12ddeb4-fd52-4a1c-a17a-af7d4fbd83ca/dask-worker-space/worker-ka_z90m5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38319', name: tcp://127.0.0.1:38319, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38319
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37233
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-a3e2591c-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-a3e2591c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-a3e2591c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-a3e2591c-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38319
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38225
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38225', name: tcp://127.0.0.1:38225, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38225
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38319', name: tcp://127.0.0.1:38319, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38319
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_pandas_series] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_pandas_series at 0x7f87759ce820>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:39929' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:788: in check_sample_weights_pandas_series
    estimator.fit(X, y, sample_weight=weights)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: in _split_to_parts
    parts = data.to_delayed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self =     0  1
0   1  1
1   1  2
2   1  3
3   1  4
4   2  1
5   2  2
6   2  3
7   2  4
8   3  1
9   3  2
10  3  3
11  3  4
name = 'to_delayed'

    def __getattr__(self, name: str):
        """
        After regular attribute access, try looking up the name
        This allows simpler access to columns for interactive use.
        """
        # Note: obj.x will always call obj.__getattribute__('x') prior to
        # calling obj.__getattr__('x').
        if (
            name in self._internal_names_set
            or name in self._metadata
            or name in self._accessors
        ):
            return object.__getattribute__(self, name)
        else:
            if self._info_axis._can_hold_identifiers_and_holds_name(name):
                return self[name]
>           return object.__getattribute__(self, name)
E           AttributeError: 'DataFrame' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/pandas/core/generic.py:5139: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:39929
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38433
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38433
distributed.worker - INFO -          dashboard at:            127.0.0.1:45753
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39929
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-ea8ce1fe-06c6-4955-99ec-80978ed8839f/dask-worker-space/worker-s1bpxkbe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32933
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32933
distributed.worker - INFO -          dashboard at:            127.0.0.1:44773
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39929
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-8d55c606-93a6-4471-90cc-3c4f304c0e64/dask-worker-space/worker-hi9aztxi
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38433', name: tcp://127.0.0.1:38433, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38433
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39929
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:32933', name: tcp://127.0.0.1:32933, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32933
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39929
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-a62d4465-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-a62d4465-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-a62d4465-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-a62d4465-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38433
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32933
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38433', name: tcp://127.0.0.1:38433, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38433
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:32933', name: tcp://127.0.0.1:32933, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:32933
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_not_an_array] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_not_an_array at 0x7f87759ce940>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:46417' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:812: in check_sample_weights_not_an_array
    estimator.fit(X, y, sample_weight=weights)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = <sklearn.utils.estimator_checks._NotAnArray object at 0x7f8767ff49a0>
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: '_NotAnArray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:46417
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38953
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38953
distributed.worker - INFO -          dashboard at:            127.0.0.1:39633
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46417
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-b8fe163e-262e-4cef-862d-ead5f9836f7d/dask-worker-space/worker-d0bulmv5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38953', name: tcp://127.0.0.1:38953, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38953
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46417
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43559
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43559
distributed.worker - INFO -          dashboard at:            127.0.0.1:46703
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46417
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-c1328052-e93b-492e-a5f2-5b0c4521bfb0/dask-worker-space/worker-r3e0qf3v
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:43559', name: tcp://127.0.0.1:43559, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43559
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46417
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-a88fd978-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-a88fd978-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-a88fd978-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-a88fd978-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43559
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38953
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38953', name: tcp://127.0.0.1:38953, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38953
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:43559', name: tcp://127.0.0.1:43559, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:43559
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_list] ____

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_list at 0x7f87759cea60>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:43757' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:829: in check_sample_weights_list
    estimator.fit(X, y, sample_weight=sample_weight)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135 , 0.71518937, 0.60276338],
       [0.54488318, 0.4236548 , 0.64589411],
       [0.43758721, 0.891773...99, 0.0641475 , 0.69247212],
       [0.56660145, 0.26538949, 0.52324805],
       [0.09394051, 0.5759465 , 0.9292962 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43757
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44253
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44253
distributed.worker - INFO -          dashboard at:            127.0.0.1:39917
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43757
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-50a7d827-0f2f-4d3a-9d4f-e52853d10f8c/dask-worker-space/worker-z1_gmr4p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35591
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35591
distributed.worker - INFO -          dashboard at:            127.0.0.1:39977
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43757
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-f9e84e9f-7361-435c-9dcc-d6b01554a566/dask-worker-space/worker-gvgalama
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44253', name: tcp://127.0.0.1:44253, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44253
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43757
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35591', name: tcp://127.0.0.1:35591, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35591
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43757
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-aaee4a6a-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-aaee4a6a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-aaee4a6a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-aaee4a6a-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35591
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35591', name: tcp://127.0.0.1:35591, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35591
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44253
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44253', name: tcp://127.0.0.1:44253, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44253
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_shape] ___

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_shape at 0x7f87759ceb80>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:33371' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:848: in check_sample_weights_shape
    estimator.fit(X, y, sample_weight=np.ones(len(y)))
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1, 3],
       [1, 3],
       [1, 3],
       [1, 3],
       [2, 1],
       [2, 1],
       [2, 1],
       [2, 1],
       [3, 3],
       [3, 3],
       [3, 3],
       [3, 3],
       [4, 1],
       [4, 1],
       [4, 1],
       [4, 1]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:33371
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39885
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39885
distributed.worker - INFO -          dashboard at:            127.0.0.1:41459
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33371
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-23f1b277-7aff-4d98-92a4-a7d0ce6d15c4/dask-worker-space/worker-_cbeh49d
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39885', name: tcp://127.0.0.1:39885, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39885
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33371
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44701
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44701
distributed.worker - INFO -          dashboard at:            127.0.0.1:45041
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33371
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-86334262-915a-4977-b720-4bfd39e4f0c1/dask-worker-space/worker-nbaij3t7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44701', name: tcp://127.0.0.1:44701, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44701
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33371
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-ad5328ff-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-ad5328ff-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-ad5328ff-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-ad5328ff-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39885
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44701
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39885', name: tcp://127.0.0.1:39885, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39885
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44701', name: tcp://127.0.0.1:44701, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44701
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_invariance] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_sample_weights_invariance at 0x7f87759ceca0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:41865' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:879: in check_sample_weights_invariance
    estimator1.fit(X, y=y, sample_weight=np.ones(shape=len(y)))
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1., 3.],
       [1., 3.],
       [1., 3.],
       [1., 3.],
       [2., 1.],
       [2., 1.],
       [2., 1.],...       [3., 3.],
       [3., 3.],
       [3., 3.],
       [4., 1.],
       [4., 1.],
       [4., 1.],
       [4., 1.]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:41865
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43645
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43645
distributed.worker - INFO -          dashboard at:            127.0.0.1:39181
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41865
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-0a6fafc2-8560-41cd-96f3-f29ffa9cf888/dask-worker-space/worker-ppue1fmh
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:43645', name: tcp://127.0.0.1:43645, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41865
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43645
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34599
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34599
distributed.worker - INFO -          dashboard at:            127.0.0.1:35847
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41865
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-26c51176-b749-4a17-ac0c-8fb945dd8817/dask-worker-space/worker-eh64doud
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34599', name: tcp://127.0.0.1:34599, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34599
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41865
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-af8ee361-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-af8ee361-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-af8ee361-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-af8ee361-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34599
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43645
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:43645', name: tcp://127.0.0.1:43645, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:43645
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34599', name: tcp://127.0.0.1:34599, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34599
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_estimators_fit_returns_self] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_fit_returns_self at 0x7f87759d7040>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:38217' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2058: in check_estimators_fit_returns_self
    assert estimator.fit(X, y) is estimator
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[5.44338915, 4.50900038],
       [4.52251198, 6.68286579],
       [5.33420024, 3.94365401],
       [6.15287792,...65598, 2.1500414 ],
       [0.45347483, 6.92854682],
       [4.97048201, 7.65863655],
       [2.93656087, 7.35343631]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:38217
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42249
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42249
distributed.worker - INFO -          dashboard at:            127.0.0.1:44377
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38217
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-aec573c6-0c3b-4439-9a25-e819a413f6c2/dask-worker-space/worker-_e4bxnf7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42249', name: tcp://127.0.0.1:42249, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42249
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38217
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33713
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33713
distributed.worker - INFO -          dashboard at:            127.0.0.1:45575
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38217
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-02bb4949-bef4-45a9-82e1-81bc2192e945/dask-worker-space/worker-q9qgu82k
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:33713', name: tcp://127.0.0.1:33713, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33713
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38217
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-b1dd8e66-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-b1dd8e66-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-b1dd8e66-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-b1dd8e66-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33713
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42249
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:33713', name: tcp://127.0.0.1:33713, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:33713
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42249', name: tcp://127.0.0.1:42249, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42249
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_estimators_fit_returns_self(readonly_memmap=True)] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_fit_returns_self at 0x7f87759d7040>, 'DaskLGBMRegressor', readonly_memmap=True)
client = <Client: 'tcp://127.0.0.1:36779' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2058: in check_estimators_fit_returns_self
    assert estimator.fit(X, y) is estimator
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = memmap([[5.44338915, 4.50900038],
        [4.52251198, 6.68286579],
        [5.33420024, 3.94365401],
        [6.15287...98, 2.1500414 ],
        [0.45347483, 6.92854682],
        [4.97048201, 7.65863655],
        [2.93656087, 7.35343631]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'memmap' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:36779
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35855
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35855
distributed.worker - INFO -          dashboard at:            127.0.0.1:36573
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36779
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-8ef2ce02-5456-4fd0-9735-7c5454effbd6/dask-worker-space/worker-b69x3qb5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35855', name: tcp://127.0.0.1:35855, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35855
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36779
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33001
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33001
distributed.worker - INFO -          dashboard at:            127.0.0.1:41405
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36779
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-3f375bc1-98d4-4ba6-a881-494dce5c35e9/dask-worker-space/worker-osbiuld2
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:33001', name: tcp://127.0.0.1:33001, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33001
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36779
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-b41e5a9e-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-b41e5a9e-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-b41e5a9e-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-b41e5a9e-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35855
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33001
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35855', name: tcp://127.0.0.1:35855, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35855
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:33001', name: tcp://127.0.0.1:33001, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:33001
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_______ test_sklearn_integration[DaskLGBMRegressor()-check_complex_data] _______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_complex_data at 0x7f87759cee50>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:34309' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:937: in check_complex_data
    assert_raises_regex(ValueError, "Complex data not supported",
/opt/conda/lib/python3.8/unittest/case.py:1357: in assertRaisesRegex
    return context.handle('assertRaisesRegex', args, kwargs)
/opt/conda/lib/python3.8/unittest/case.py:202: in handle
    callable_obj(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:34309
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33779
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33779
distributed.worker - INFO -          dashboard at:            127.0.0.1:41193
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34309
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-f08b49fe-de93-41ea-ac64-100f3b558ecb/dask-worker-space/worker-fsse39t_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39907
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39907
distributed.worker - INFO -          dashboard at:            127.0.0.1:35699
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34309
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-2b2d50c4-a555-49cd-a107-46eb0dd38905/dask-worker-space/worker-uh32nfa4
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:33779', name: tcp://127.0.0.1:33779, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33779
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34309
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39907', name: tcp://127.0.0.1:39907, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39907
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34309
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-b67aeb81-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-b67aeb81-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-b67aeb81-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-b67aeb81-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33779
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39907
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:33779', name: tcp://127.0.0.1:33779, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:33779
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39907', name: tcp://127.0.0.1:39907, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39907
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_______ test_sklearn_integration[DaskLGBMRegressor()-check_dtype_object] _______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_dtype_object at 0x7f87759cedc0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:44637' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:906: in check_dtype_object
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135039273248, 0.7151893663724195, 0.6027633760716439,
        0.5448831829968969, 0.4236547993389047, 0....4736,
        0.3553688484719296, 0.3567068904025429, 0.01632850268370789,
        0.18523232523618394]], dtype=object)
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:44637
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34227
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34227
distributed.worker - INFO -          dashboard at:            127.0.0.1:46767
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44637
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-dfd360df-7c13-47fd-bc70-dcb47788c049/dask-worker-space/worker-j177z_qr
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34227', name: tcp://127.0.0.1:34227, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34227
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44637
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34591
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34591
distributed.worker - INFO -          dashboard at:            127.0.0.1:37249
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44637
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-f39a5ef6-832e-48b0-aa8a-edf1e2bfaf89/dask-worker-space/worker-h6o6o25y
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34591', name: tcp://127.0.0.1:34591, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34591
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44637
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-b8c5f2c5-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-b8c5f2c5-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-b8c5f2c5-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-b8c5f2c5-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34227
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34591
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34227', name: tcp://127.0.0.1:34227, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34227
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34591', name: tcp://127.0.0.1:34591, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34591
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_estimators_empty_data_messages] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_empty_data_messages at 0x7f87759d4160>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:42169' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1427: in check_estimators_empty_data_messages
    e.fit(X_zero_samples, [])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:42169
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34249
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34249
distributed.worker - INFO -          dashboard at:            127.0.0.1:34393
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42169
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-abcff367-6523-4a99-a929-05ad34362287/dask-worker-space/worker-rjbtdgi9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40963
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40963
distributed.worker - INFO -          dashboard at:            127.0.0.1:41321
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42169
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-5ec3e355-452b-49a1-9071-f0f280bd55e9/dask-worker-space/worker-5o2s36c7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34249', name: tcp://127.0.0.1:34249, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34249
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42169
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40963', name: tcp://127.0.0.1:40963, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40963
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42169
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-bb44afc7-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-bb44afc7-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-bb44afc7-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-bb44afc7-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40963
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34249
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34249', name: tcp://127.0.0.1:34249, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34249
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40963', name: tcp://127.0.0.1:40963, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40963
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMRegressor()-check_pipeline_consistency] ___

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_pipeline_consistency at 0x7f87759d0dc0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:43713' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1349: in check_pipeline_consistency
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.26049352, 1.32820804, 1.26819727],
       [0.37832805, 0.37553697, 0.2165663 ],
       [0.29635883, 0.269703...5 , 1.14822372, 1.36074415],
       [1.18681797, 1.16821927, 1.19741402],
       [0.22506871, 0.15044369, 0.11329719]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43713
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38917
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38917
distributed.worker - INFO -          dashboard at:            127.0.0.1:36683
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43713
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-0e8a38b4-1697-41d7-81da-b187569be070/dask-worker-space/worker-sgx2lfet
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38917', name: tcp://127.0.0.1:38917, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38917
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43713
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34499
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34499
distributed.worker - INFO -          dashboard at:            127.0.0.1:43449
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43713
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-b10dd1d4-7f59-4a5c-88e4-89b2105c1278/dask-worker-space/worker-_61fx5ve
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34499', name: tcp://127.0.0.1:34499, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34499
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43713
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-bd8696db-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-bd8696db-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-bd8696db-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-bd8696db-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34499
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38917
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38917', name: tcp://127.0.0.1:38917, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38917
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34499', name: tcp://127.0.0.1:34499, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34499
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_estimators_overwrite_params] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_overwrite_params at 0x7f87759d7dc0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:33895' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2427: in check_estimators_overwrite_params
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[5.44338915, 4.50900038],
       [4.52251198, 6.68286579],
       [5.33420024, 3.94365401],
       [6.15287792,...65598, 2.1500414 ],
       [0.45347483, 6.92854682],
       [4.97048201, 7.65863655],
       [2.93656087, 7.35343631]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:33895
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34447
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34447
distributed.worker - INFO -          dashboard at:            127.0.0.1:35897
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33895
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-68d18c13-92e3-4b0f-be87-0863ae682b9e/dask-worker-space/worker-iqdxadmz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41343
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41343
distributed.worker - INFO -          dashboard at:            127.0.0.1:43399
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33895
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-24b5e219-691f-4c30-920b-978616d2057d/dask-worker-space/worker-hvf3rck2
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34447', name: tcp://127.0.0.1:34447, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33895
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34447
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:41343', name: tcp://127.0.0.1:41343, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41343
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33895
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-bffcd79e-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-bffcd79e-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-bffcd79e-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-bffcd79e-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41343
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34447
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34447', name: tcp://127.0.0.1:34447, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34447
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:41343', name: tcp://127.0.0.1:41343, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:41343
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
__ test_sklearn_integration[DaskLGBMRegressor()-check_estimator_sparse_data] ___

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimator_sparse_data at 0x7f87759ce700>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:42701' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:737: in check_estimator_sparse_data
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: in _split_to_parts
    parts = data.to_delayed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <40x10 sparse matrix of type '<class 'numpy.float64'>'
	with 74 stored elements in Compressed Sparse Row format>
attr = 'to_delayed'

    def __getattr__(self, attr):
        if attr == 'A':
            return self.toarray()
        elif attr == 'T':
            return self.transpose()
        elif attr == 'H':
            return self.getH()
        elif attr == 'real':
            return self._real()
        elif attr == 'imag':
            return self._imag()
        elif attr == 'size':
            return self.getnnz()
        else:
>           raise AttributeError(attr + " not found")
E           AttributeError: to_delayed not found

/opt/conda/lib/python3.8/site-packages/scipy/sparse/base.py:687: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:42701
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46883
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46883
distributed.worker - INFO -          dashboard at:            127.0.0.1:37807
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42701
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-c60020f4-f479-491e-ae18-2b9a82bfe33e/dask-worker-space/worker-t8i0zopt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41181
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41181
distributed.worker - INFO -          dashboard at:            127.0.0.1:37503
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42701
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-cc2ee91b-1056-4cfc-9e03-88dd74cd71ed/dask-worker-space/worker-yarfek1r
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:46883', name: tcp://127.0.0.1:46883, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46883
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:41181', name: tcp://127.0.0.1:41181, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41181
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-c2491c47-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
----------------------------- Captured stdout call -----------------------------
Estimator DaskLGBMRegressor doesn't seem to fail gracefully on sparse data: it should raise a TypeError if sparse input is explicitly not supported.
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-c2491c47-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-c2491c47-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-c2491c47-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41181
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46883
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:41181', name: tcp://127.0.0.1:41181, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:41181
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:46883', name: tcp://127.0.0.1:46883, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:46883
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
____ test_sklearn_integration[DaskLGBMRegressor()-check_estimators_pickle] _____

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_pickle at 0x7f87759d44c0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:43699' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1548: in check_estimators_pickle
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[       nan, 1.32820804, 1.26819727],
       [0.37832805, 0.37553697, 0.2165663 ],
       [0.29635883, 0.269703...5 , 1.14822372, 1.36074415],
       [1.18681797, 1.16821927, 1.19741402],
       [0.22506871, 0.15044369, 0.11329719]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43699
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38509
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38509
distributed.worker - INFO -          dashboard at:            127.0.0.1:38639
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43699
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-8775be25-71a2-4ed8-a946-de44f88be30f/dask-worker-space/worker-y8cmycon
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38509', name: tcp://127.0.0.1:38509, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38509
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43699
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42417
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42417
distributed.worker - INFO -          dashboard at:            127.0.0.1:36771
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43699
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-008981a9-9132-4648-a473-812bb08b6a9c/dask-worker-space/worker-zspnmecf
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42417', name: tcp://127.0.0.1:42417, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42417
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43699
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-c4d47a96-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-c4d47a96-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-c4d47a96-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-c4d47a96-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38509
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42417
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38509', name: tcp://127.0.0.1:38509, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38509
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42417', name: tcp://127.0.0.1:42417, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42417
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMRegressor()-check_regressors_train] _____

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_regressors_train at 0x7f87759d7820>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:45093' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2266: in check_regressors_train
    regressor.fit(X, y[:-1])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:45093
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40899
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40899
distributed.worker - INFO -          dashboard at:            127.0.0.1:33201
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45093
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-4685bab7-1605-41c2-a5fd-e08e87301f2d/dask-worker-space/worker-fkudr9zb
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40899', name: tcp://127.0.0.1:40899, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40899
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45093
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38501
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38501
distributed.worker - INFO -          dashboard at:            127.0.0.1:42547
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45093
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-3eb65751-c9d2-484b-a75e-f2bc030786d9/dask-worker-space/worker-90a5kvl7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38501', name: tcp://127.0.0.1:38501, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38501
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45093
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-c7441381-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-c7441381-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-c7441381-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-c7441381-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40899
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38501
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40899', name: tcp://127.0.0.1:40899, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40899
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38501', name: tcp://127.0.0.1:38501, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38501
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_regressors_train(readonly_memmap=True)] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_regressors_train at 0x7f87759d7820>, 'DaskLGBMRegressor', readonly_memmap=True)
client = <Client: 'tcp://127.0.0.1:36239' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2266: in check_regressors_train
    regressor.fit(X, y[:-1])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'memmap' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:36239
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42443
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42443
distributed.worker - INFO -          dashboard at:            127.0.0.1:40043
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36239
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-38e3dacf-d47d-4ae2-9d5c-490e7ec74d23/dask-worker-space/worker-u2tmomor
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:42443', name: tcp://127.0.0.1:42443, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42443
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36239
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36965
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36965
distributed.worker - INFO -          dashboard at:            127.0.0.1:33507
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36239
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-9d527638-b057-4a72-9cce-81cb3d074c5b/dask-worker-space/worker-gw26kbta
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36965', name: tcp://127.0.0.1:36965, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36965
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36239
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-c9948491-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-c9948491-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-c9948491-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-c9948491-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36965
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42443
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36965', name: tcp://127.0.0.1:36965, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36965
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:42443', name: tcp://127.0.0.1:42443, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:42443
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_regressors_train(readonly_memmap=True,X_dtype=float32)] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_regressors_train at 0x7f87759d7820>, 'DaskLGBMRegressor', readonly_memmap=True, X_dtype='float32')
client = <Client: 'tcp://127.0.0.1:43239' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2266: in check_regressors_train
    regressor.fit(X, y[:-1])
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'memmap' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43239
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41923
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41923
distributed.worker - INFO -          dashboard at:            127.0.0.1:41671
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43239
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-8d0540e0-b022-4f5d-883f-8ecc98f538fb/dask-worker-space/worker-rvbb6m0x
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:41923', name: tcp://127.0.0.1:41923, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43239
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41923
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43385
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43385
distributed.worker - INFO -          dashboard at:            127.0.0.1:43289
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43239
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-1a8add2f-dc59-4778-bcda-1807b86ac2eb/dask-worker-space/worker-25vrbw8w
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:43385', name: tcp://127.0.0.1:43385, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43385
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43239
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-cbf1875d-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-cbf1875d-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-cbf1875d-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-cbf1875d-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43385
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:43385', name: tcp://127.0.0.1:43385, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:43385
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41923
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:41923', name: tcp://127.0.0.1:41923, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:41923
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_regressor_data_not_an_array] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_regressor_data_not_an_array at 0x7f87759db280>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:46693' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2522: in check_regressor_data_not_an_array
    check_estimators_data_not_an_array(name, estimator_orig, X, y,
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2562: in check_estimators_data_not_an_array
    estimator_1.fit(X_, y_)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = <sklearn.utils.estimator_checks._NotAnArray object at 0x7f877473de20>
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: '_NotAnArray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:46693
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38491
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38491
distributed.worker - INFO -          dashboard at:            127.0.0.1:44453
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46693
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-e696fe9b-800a-4556-8fb2-009f0dcae977/dask-worker-space/worker-cl_gp4qg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44527
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44527
distributed.worker - INFO -          dashboard at:            127.0.0.1:40487
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46693
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-74278777-608f-4490-8362-cc38f9225505/dask-worker-space/worker-53po0m5l
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38491', name: tcp://127.0.0.1:38491, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38491
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46693
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44527', name: tcp://127.0.0.1:44527, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44527
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46693
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-ce1fe2b3-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-ce1fe2b3-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-ce1fe2b3-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-ce1fe2b3-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44527
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38491
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44527', name: tcp://127.0.0.1:44527, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44527
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38491', name: tcp://127.0.0.1:38491, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38491
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_regressors_no_decision_function] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_regressors_no_decision_function at 0x7f87759d7940>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:34335' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2295: in check_regressors_no_decision_function
    regressor.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ],
       [ 1.86755799, -0.97727788,  0.95008842, -0.1513572...    [-0.88778575, -1.98079647, -0.34791215,  0.15634897],
       [ 1.23029068,  1.20237985, -0.38732682, -0.30230275]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:34335
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37043
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37043
distributed.worker - INFO -          dashboard at:            127.0.0.1:46863
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34335
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-7d8a5200-f914-462c-ae91-67ab5b212c65/dask-worker-space/worker-pq857qya
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40593
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40593
distributed.worker - INFO -          dashboard at:            127.0.0.1:45413
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34335
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-d245fa78-8fd6-45ad-9b45-b4c1182e79ea/dask-worker-space/worker-9hncpvlz
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37043', name: tcp://127.0.0.1:37043, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37043
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34335
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40593', name: tcp://127.0.0.1:40593, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40593
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:34335
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-d2ca7bdf-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-d2ca7bdf-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-d2ca7bdf-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-d2ca7bdf-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37043
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40593
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40593', name: tcp://127.0.0.1:40593, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40593
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37043', name: tcp://127.0.0.1:37043, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37043
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMRegressor()-check_supervised_y_2d] ______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_supervised_y_2d at 0x7f87759d7280>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:44889' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2093: in check_supervised_y_2d
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[0.5488135 , 0.71518937, 0.60276338],
       [0.54488318, 0.4236548 , 0.64589411],
       [0.43758721, 0.891773...99, 0.0641475 , 0.69247212],
       [0.56660145, 0.26538949, 0.52324805],
       [0.09394051, 0.5759465 , 0.9292962 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:44889
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38667
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38667
distributed.worker - INFO -          dashboard at:            127.0.0.1:33107
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44889
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-e1057276-0d38-4cfd-bb9f-f987e3529340/dask-worker-space/worker-kuuj4o80
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38667', name: tcp://127.0.0.1:38667, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38667
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44889
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39479
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39479
distributed.worker - INFO -          dashboard at:            127.0.0.1:44775
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44889
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-6fe7529f-6c2c-4e0a-bbff-3012198f0759/dask-worker-space/worker-_rbjyed7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39479', name: tcp://127.0.0.1:39479, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39479
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44889
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-d5205eb5-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-d5205eb5-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-d5205eb5-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-d5205eb5-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38667
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39479
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39479', name: tcp://127.0.0.1:39479, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39479
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38667', name: tcp://127.0.0.1:38667, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38667
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMRegressor()-check_supervised_y_no_nan] ____

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_supervised_y_no_nan at 0x7f87759cd790>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:46807' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:149: in check_supervised_y_no_nan
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[-1.76200874e-01,  1.88876361e-01,  8.26747180e-01,
        -3.24473146e-02, -6.52499418e-01],
       [-1.05339...1.06098418e+00],
       [ 1.31845313e-01,  1.71934297e-01,  8.88062794e-02,
         1.03963016e+00, -1.37846942e-01]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:46807
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40309
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40309
distributed.worker - INFO -          dashboard at:            127.0.0.1:34747
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46807
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-7a2855be-07ee-4fef-b916-eeffa1c4b048/dask-worker-space/worker-xezx9wgv
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40309', name: tcp://127.0.0.1:40309, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40309
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46807
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45183
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45183
distributed.worker - INFO -          dashboard at:            127.0.0.1:39987
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46807
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-449918d6-f452-4dc8-9508-108210e4ecae/dask-worker-space/worker-w_z7uh1m
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:45183', name: tcp://127.0.0.1:45183, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45183
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46807
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-d78f9815-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-d78f9815-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-d78f9815-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-d78f9815-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40309
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45183
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:45183', name: tcp://127.0.0.1:45183, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:45183
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40309', name: tcp://127.0.0.1:40309, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40309
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMRegressor()-check_regressors_int] ______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_regressors_int at 0x7f87759d7700>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:45645' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2227: in check_regressors_int
    regressor_1.fit(X, y_)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[-0.45364538, -0.47282444, -1.20608008, -0.27435163, -0.81468082,
         0.11961125, -1.82270409,  0.63188816...       0.69828059,  0.44556918, -0.50849021, -0.56448069, -0.20145575,
         1.17719428,  0.44630453, -0.48981989]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:45645
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46003
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46003
distributed.worker - INFO -          dashboard at:            127.0.0.1:38817
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45645
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-88609218-45eb-461e-be8a-480a8a0071ac/dask-worker-space/worker-u2_zjp8f
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:46003', name: tcp://127.0.0.1:46003, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46003
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45645
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44173
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44173
distributed.worker - INFO -          dashboard at:            127.0.0.1:41985
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45645
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-f7166ea4-5d54-40cc-9bcd-b79c1b851a19/dask-worker-space/worker-8nwniszl
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44173', name: tcp://127.0.0.1:44173, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44173
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45645
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-da0d6729-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-da0d6729-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-da0d6729-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-da0d6729-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46003
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44173
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:46003', name: tcp://127.0.0.1:46003, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:46003
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44173', name: tcp://127.0.0.1:44173, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44173
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
___ test_sklearn_integration[DaskLGBMRegressor()-check_estimators_unfitted] ____

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_estimators_unfitted at 0x7f87759d7160>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:39519' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2074: in check_estimators_unfitted
    assert_raises(NotFittedError, getattr(estimator, method), X)
/opt/conda/lib/python3.8/unittest/case.py:816: in assertRaises
    return context.handle('assertRaises', args, kwargs)
/opt/conda/lib/python3.8/unittest/case.py:202: in handle
    callable_obj(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:788: in predict
    return _predict(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _predict(
        model: LGBMModel,
        data: _DaskMatrixLike,
        raw_score: bool = False,
        pred_proba: bool = False,
        pred_leaf: bool = False,
        pred_contrib: bool = False,
        dtype: _PredictionDtype = np.float32,
        **kwargs: Any
    ) -> dask_Array:
        """Inner predict routine.
    
        Parameters
        ----------
        model : lightgbm.LGBMClassifier, lightgbm.LGBMRegressor, or lightgbm.LGBMRanker class
            Fitted underlying model.
        data : Dask Array or Dask DataFrame of shape = [n_samples, n_features]
            Input feature matrix.
        raw_score : bool, optional (default=False)
            Whether to predict raw scores.
        pred_proba : bool, optional (default=False)
            Should method return results of ``predict_proba`` (``pred_proba=True``) or ``predict`` (``pred_proba=False``).
        pred_leaf : bool, optional (default=False)
            Whether to predict leaf index.
        pred_contrib : bool, optional (default=False)
            Whether to predict feature contributions.
        dtype : np.dtype, optional (default=np.float32)
            Dtype of the output.
        **kwargs
            Other parameters passed to ``predict`` or ``predict_proba`` method.
    
        Returns
        -------
        predicted_result : Dask Array of shape = [n_samples] or shape = [n_samples, n_classes]
            The predicted values.
        X_leaves : Dask Array of shape = [n_samples, n_trees] or shape = [n_samples, n_trees * n_classes]
            If ``pred_leaf=True``, the predicted leaf of every tree for each sample.
        X_SHAP_values : Dask Array of shape = [n_samples, n_features + 1] or shape = [n_samples, (n_features + 1) * n_classes]
            If ``pred_contrib=True``, the feature contributions for each sample.
        """
        if not all((DASK_INSTALLED, PANDAS_INSTALLED, SKLEARN_INSTALLED)):
            raise LightGBMError('dask, pandas and scikit-learn are required for lightgbm.dask')
        if isinstance(data, dask_DataFrame):
            return data.map_partitions(
                _predict_part,
                model=model,
                raw_score=raw_score,
                pred_proba=pred_proba,
                pred_leaf=pred_leaf,
                pred_contrib=pred_contrib,
                **kwargs
            ).values
        elif isinstance(data, dask_Array):
            if pred_proba:
                kwargs['chunks'] = (data.chunks[0], (model.n_classes_,))
            else:
                kwargs['drop_axis'] = 1
            return data.map_blocks(
                _predict_part,
                model=model,
                raw_score=raw_score,
                pred_proba=pred_proba,
                pred_leaf=pred_leaf,
                pred_contrib=pred_contrib,
                dtype=dtype,
                **kwargs
            )
        else:
>           raise TypeError('Data must be either Dask Array or Dask DataFrame. Got %s.' % str(type(data)))
E           TypeError: Data must be either Dask Array or Dask DataFrame. Got <class 'numpy.ndarray'>.

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:457: TypeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:39519
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40995
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40995
distributed.worker - INFO -          dashboard at:            127.0.0.1:39187
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39519
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-9df2413b-61f3-4bc8-b336-24edfe7ef224/dask-worker-space/worker-asvahu35
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40995', name: tcp://127.0.0.1:40995, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40995
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39519
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36455
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36455
distributed.worker - INFO -          dashboard at:            127.0.0.1:35339
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39519
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-177208fe-198d-4ad8-b45e-d359415a8780/dask-worker-space/worker-3b_ndrzm
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:36455', name: tcp://127.0.0.1:36455, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36455
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39519
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-dc6bd86a-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-dc6bd86a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-dc6bd86a-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-dc6bd86a-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40995
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36455
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40995', name: tcp://127.0.0.1:40995, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40995
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:36455', name: tcp://127.0.0.1:36455, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:36455
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_____ test_sklearn_integration[DaskLGBMRegressor()-check_fit2d_predict1d] ______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit2d_predict1d at 0x7f87759d03a0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:41287' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1061: in check_fit2d_predict1d
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013],
       [1.63464955, 1.2709644 , 1.93768234],
       [1.31276163, 2.675319...54, 2.96512151, 0.30613443],
       [0.62663027, 0.48392855, 1.95932498],
       [0.75987481, 1.39893232, 0.73327678]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:41287
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37201
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37201
distributed.worker - INFO -          dashboard at:            127.0.0.1:43439
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-f005f9cd-8e8f-4105-8fe7-1011e5a2172e/dask-worker-space/worker-_oc6q1wc
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:37201', name: tcp://127.0.0.1:37201, memory: 0, processing: 0>
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41287
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37201
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32935
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32935
distributed.worker - INFO -          dashboard at:            127.0.0.1:37473
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-c1d60e25-57c4-4ff6-8b3d-1618b2fac717/dask-worker-space/worker-2457uway
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:32935', name: tcp://127.0.0.1:32935, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32935
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41287
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-e14de280-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-e14de280-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-e14de280-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-e14de280-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32935
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37201
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:32935', name: tcp://127.0.0.1:32935, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:32935
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:37201', name: tcp://127.0.0.1:37201, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:37201
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_methods_subset_invariance] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_methods_subset_invariance at 0x7f87759d0550>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:42477' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1109: in check_methods_subset_invariance
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013],
       [1.63464955, 1.2709644 , 1.93768234],
       [1.31276163, 2.675319...54, 2.96512151, 0.30613443],
       [0.62663027, 0.48392855, 1.95932498],
       [0.75987481, 1.39893232, 0.73327678]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:42477
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43989
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43989
distributed.worker - INFO -          dashboard at:            127.0.0.1:37051
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42477
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-893892d1-24e5-4fc5-ba8e-983369ab3d89/dask-worker-space/worker-f5_2f5t2
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:43989', name: tcp://127.0.0.1:43989, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43989
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42477
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34911
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34911
distributed.worker - INFO -          dashboard at:            127.0.0.1:44909
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42477
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-df041c65-15bc-44ca-87fc-6c734c4db1f6/dask-worker-space/worker-s9jcmihr
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34911', name: tcp://127.0.0.1:34911, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34911
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42477
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-e3ad61a6-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-e3ad61a6-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-e3ad61a6-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-e3ad61a6-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34911
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43989
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34911', name: tcp://127.0.0.1:34911, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34911
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:43989', name: tcp://127.0.0.1:43989, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:43989
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMRegressor()-check_fit2d_1sample] _______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit2d_1sample at 0x7f87759d0670>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:36631' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1152: in check_fit2d_1sample
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013, 1.63464955, 1.2709644 ,
        1.93768234, 1.31276163, 2.675319  , 2.89098828, 1.15032456]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:36631
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34103
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34103
distributed.worker - INFO -          dashboard at:            127.0.0.1:39263
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36631
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-b38edb2e-300e-438f-86b4-4cd68ac38200/dask-worker-space/worker-lnaiz_lh
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:34103', name: tcp://127.0.0.1:34103, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34103
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36631
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35159
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35159
distributed.worker - INFO -          dashboard at:            127.0.0.1:43223
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36631
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-b4262e0f-f4d7-442d-9d61-0e673b33cbc4/dask-worker-space/worker-gaub9en7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:35159', name: tcp://127.0.0.1:35159, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35159
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36631
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-e603ba54-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-e603ba54-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-e603ba54-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-e603ba54-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35159
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34103
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:35159', name: tcp://127.0.0.1:35159, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:35159
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:34103', name: tcp://127.0.0.1:34103, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:34103
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMRegressor()-check_fit2d_1feature] ______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit2d_1feature at 0x7f87759d0790>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:45983' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1186: in check_fit2d_1feature
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051],
       [2.1455681 ],
       [1.80829013],
       [1.63464955],
       [1.2709644 ],
       [1.93768234],
       [1.31276163],
       [2.675319  ],
       [2.89098828],
       [1.15032456]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:45983
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39991
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39991
distributed.worker - INFO -          dashboard at:            127.0.0.1:34403
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45983
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-33695f45-7e56-4db9-84f2-733f8dd55af9/dask-worker-space/worker-flvo5hu0
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:39991', name: tcp://127.0.0.1:39991, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40243
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40243
distributed.worker - INFO -          dashboard at:            127.0.0.1:44673
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45983
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39991
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-ca93f366-2411-4a4d-a22e-7dd33615bf15/dask-worker-space/worker-j0nj6w16
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45983
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40243', name: tcp://127.0.0.1:40243, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40243
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45983
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-e851407e-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-e851407e-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-e851407e-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-e851407e-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39991
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40243
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40243', name: tcp://127.0.0.1:40243, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40243
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:39991', name: tcp://127.0.0.1:39991, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:39991
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
__________ test_sklearn_integration[DaskLGBMRegressor()-check_fit1d] ___________

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit1d at 0x7f87759d08b0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:32777' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1211: in check_fit1d
    assert_raises(ValueError, estimator.fit, X, y)
/opt/conda/lib/python3.8/unittest/case.py:816: in assertRaises
    return context.handle('assertRaises', args, kwargs)
/opt/conda/lib/python3.8/unittest/case.py:202: in handle
    callable_obj(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:32777
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40249
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40249
distributed.worker - INFO -          dashboard at:            127.0.0.1:37895
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32777
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-95c502ad-542b-4c3e-8c2b-481026480a84/dask-worker-space/worker-m5ytq9lg
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40249', name: tcp://127.0.0.1:40249, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40249
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32777
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44515
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44515
distributed.worker - INFO -          dashboard at:            127.0.0.1:33657
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32777
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-b79a9390-1bd7-4338-ae1e-69ed17d21ecb/dask-worker-space/worker-xff31n_5
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44515', name: tcp://127.0.0.1:44515, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44515
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:32777
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-eadccc20-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-eadccc20-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-eadccc20-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-eadccc20-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40249
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44515
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40249', name: tcp://127.0.0.1:40249, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40249
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44515', name: tcp://127.0.0.1:44515, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44515
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMRegressor()-check_dict_unchanged] ______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_dict_unchanged at 0x7f87759cef70>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:35495' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:971: in check_dict_unchanged
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.09762701, 1.43037873, 1.20552675],
       [1.08976637, 0.8473096 , 1.29178823],
       [0.87517442, 1.783546...03, 1.97674768, 0.20408962],
       [0.41775351, 0.32261904, 1.30621665],
       [0.50658321, 0.93262155, 0.48885118]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:35495
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40065
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40065
distributed.worker - INFO -          dashboard at:            127.0.0.1:43633
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35495
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-3fbf4362-2b71-4bbf-9f4b-89e6f9524992/dask-worker-space/worker-_5gtucvj
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40065', name: tcp://127.0.0.1:40065, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40065
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35495
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40645
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40645
distributed.worker - INFO -          dashboard at:            127.0.0.1:46155
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35495
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-de5436c5-aec5-408a-a8c6-beca06f8252b/dask-worker-space/worker-f3w4juln
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40645', name: tcp://127.0.0.1:40645, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40645
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35495
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-f1b6f4d0-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-f1b6f4d0-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-f1b6f4d0-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-f1b6f4d0-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40645
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40065
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40065', name: tcp://127.0.0.1:40065, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40065
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40645', name: tcp://127.0.0.1:40645, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40645
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
_ test_sklearn_integration[DaskLGBMRegressor()-check_dont_overwrite_parameters] _

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_dont_overwrite_parameters at 0x7f87759d0280>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:46345' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/_testing.py:317: in wrapper
    return fn(*args, **kwargs)
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:1011: in check_dont_overwrite_parameters
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[1.64644051, 2.1455681 , 1.80829013],
       [1.63464955, 1.2709644 , 1.93768234],
       [1.31276163, 2.675319...54, 2.96512151, 0.30613443],
       [0.62663027, 0.48392855, 1.95932498],
       [0.75987481, 1.39893232, 0.73327678]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:46345
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46523
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46523
distributed.worker - INFO -          dashboard at:            127.0.0.1:44059
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46345
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-43f543a1-4a1a-4dee-a9f6-115d3ed5f662/dask-worker-space/worker-diajjb7u
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:46523', name: tcp://127.0.0.1:46523, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46523
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46345
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38911
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38911
distributed.worker - INFO -          dashboard at:            127.0.0.1:34715
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:46345
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-d06ed87f-b756-4615-aef9-0f330ae06ef1/dask-worker-space/worker-3vyyyvm7
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:38911', name: tcp://127.0.0.1:38911, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38911
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:46345
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-f423e54c-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-f423e54c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-f423e54c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-f423e54c-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46523
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38911
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:38911', name: tcp://127.0.0.1:38911, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:38911
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:46523', name: tcp://127.0.0.1:46523, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:46523
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMRegressor()-check_fit_idempotent] ______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_fit_idempotent at 0x7f87759dbee0>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:42889' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2930: in check_fit_idempotent
    estimator.fit(X_train, y_train)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[ 99.12920285,  99.42115034],
       [100.57659082,  99.79170124],
       [ 98.96575716, 100.68159452],
       ...63904],
       [101.8831507 ,  98.65224094],
       [ 98.89561666, 100.05216508],
       [ 99.18685372,  98.2737174 ]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:42889
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44653
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44653
distributed.worker - INFO -          dashboard at:            127.0.0.1:45127
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42889
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-27f33731-01b6-4bda-8f62-856469e1ef54/dask-worker-space/worker-8nf8qisn
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44653', name: tcp://127.0.0.1:44653, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44653
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42889
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40773
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40773
distributed.worker - INFO -          dashboard at:            127.0.0.1:40337
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42889
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-110a1ee5-dd5a-454a-9365-61cc0a7b98ce/dask-worker-space/worker-63j34f87
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40773', name: tcp://127.0.0.1:40773, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40773
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42889
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-f6645427-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-f6645427-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-f6645427-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-f6645427-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44653
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40773
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44653', name: tcp://127.0.0.1:44653, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44653
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40773', name: tcp://127.0.0.1:40773, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40773
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
______ test_sklearn_integration[DaskLGBMRegressor()-check_n_features_in] _______

estimator = DaskLGBMRegressor(local_listen_port=18000, time_out=5)
check = functools.partial(<function check_n_features_in at 0x7f87759dbf70>, 'DaskLGBMRegressor')
client = <Client: 'tcp://127.0.0.1:43747' processes=2 threads=2, memory=4.17 GB>

    @parametrize_with_checks(list(_tested_estimators()))
    def test_sklearn_integration(estimator, check, client):
        estimator.set_params(local_listen_port=18000, time_out=5)
>       check(estimator)

test_dask.py:1126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/conda/lib/python3.8/site-packages/sklearn/utils/estimator_checks.py:2975: in check_n_features_in
    estimator.fit(X, y)
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:760: in fit
    return self._fit(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:498: in _fit
    model = _train(
/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:288: in _train
    data_parts = _split_to_parts(data=data, is_matrix=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

data = array([[101.76405235, 100.40015721],
       [100.97873798, 102.2408932 ],
       [101.86755799,  99.02272212],
       ...9065 ],
       [100.52327666,  99.82845367],
       [100.77179055, 100.82350415],
       [102.16323595, 101.33652795]])
is_matrix = True

    def _split_to_parts(data: _DaskCollection, is_matrix: bool) -> List[_DaskPart]:
>       parts = data.to_delayed()
E       AttributeError: 'numpy.ndarray' object has no attribute 'to_delayed'

/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:199: AttributeError
---------------------------- Captured stderr setup -----------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:     tcp://127.0.0.1:43747
distributed.scheduler - INFO -   dashboard at:            127.0.0.1:8787
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40775
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40775
distributed.worker - INFO -          dashboard at:            127.0.0.1:37713
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43747
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-5f538e94-c284-4280-baf5-417d7d82f4d0/dask-worker-space/worker-1gv7qd_6
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:40775', name: tcp://127.0.0.1:40775, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40775
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43747
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44749
distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44749
distributed.worker - INFO -          dashboard at:            127.0.0.1:39155
distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43747
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    2.08 GB
distributed.worker - INFO -       Local Directory: /opt/LightGBM/tests/python_package_test/_test_worker-82e9ec1f-8594-4614-b2b5-72b5c13548fb/dask-worker-space/worker-qpoqxvfc
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <Worker 'tcp://127.0.0.1:44749', name: tcp://127.0.0.1:44749, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44749
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43747
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-f8b8bf9c-6f4c-11eb-8a1b-0242ac110002
distributed.core - INFO - Starting established connection
--------------------------- Captured stderr teardown ---------------------------
distributed.scheduler - INFO - Remove client Client-f8b8bf9c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Remove client Client-f8b8bf9c-6f4c-11eb-8a1b-0242ac110002
distributed.scheduler - INFO - Close client connection: Client-f8b8bf9c-6f4c-11eb-8a1b-0242ac110002
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40775
distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44749
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:40775', name: tcp://127.0.0.1:40775, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:40775
distributed.scheduler - INFO - Remove worker <Worker 'tcp://127.0.0.1:44749', name: tcp://127.0.0.1:44749, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://127.0.0.1:44749
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
=========================== short test summary info ============================
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimators_dtypes]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_fit_score_takes_y]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_pandas_series]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_not_an_array]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_list]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_shape]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_sample_weights_invariance]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimators_fit_returns_self]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimators_fit_returns_self(readonly_memmap=True)]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_complex_data]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_dtype_object]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimators_empty_data_messages]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_pipeline_consistency]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimators_overwrite_params]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimator_sparse_data]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimators_pickle]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_classifier_data_not_an_array]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_one_label]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_classes]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_train]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_train(readonly_memmap=True)]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_train(readonly_memmap=True,X_dtype=float32)]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_classifiers_regression_target]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_supervised_y_no_nan]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_supervised_y_2d]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_estimators_unfitted]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_class_weight_classifiers]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_fit2d_predict1d]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_methods_subset_invariance]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_fit2d_1sample]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_fit2d_1feature]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_fit1d]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_dict_unchanged]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_dont_overwrite_parameters]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_fit_idempotent]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMClassifier()-check_n_features_in]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimators_dtypes]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_fit_score_takes_y]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_pandas_series]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_not_an_array]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_list]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_shape]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_sample_weights_invariance]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimators_fit_returns_self]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimators_fit_returns_self(readonly_memmap=True)]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_complex_data]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_dtype_object]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimators_empty_data_messages]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_pipeline_consistency]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimators_overwrite_params]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimator_sparse_data]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimators_pickle]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_regressors_train]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_regressors_train(readonly_memmap=True)]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_regressors_train(readonly_memmap=True,X_dtype=float32)]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_regressor_data_not_an_array]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_regressors_no_decision_function]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_supervised_y_2d]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_supervised_y_no_nan]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_regressors_int]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_estimators_unfitted]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_fit2d_predict1d]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_methods_subset_invariance]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_fit2d_1sample]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_fit2d_1feature]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_fit1d]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_dict_unchanged]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_dont_overwrite_parameters]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_fit_idempotent]
FAILED test_dask.py::test_sklearn_integration[DaskLGBMRegressor()-check_n_features_in]
============= 70 failed, 9 passed, 2 xfailed in 326.35s (0:05:26) ==============
