---
title:
  "Basic Walkthrough"
description: >
  This vignette illustrates the basics of LightGBM.
bibliography: "biblio.bib"
link-citations: true
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Basic Walkthrough}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  , collapse = TRUE
  , comment = "#>"
  , warning = FALSE
  message = FALSE
)
```

## Introduction

Welcome to the world of [LightGBM](https://lightgbm.readthedocs.io/en/latest/), a hightly efficient gradient boosting implementation [@ke2017]. 

```{r setup}
library(lightgbm)
```

This vignette will guide you through its basic usage. For illustration, we will use a subset of the `bank` dataset [@moro2014] to predict whether a client has subscribed a term deposit. As input features, we will use two numeric predictors, "age" and "balance". 

## The dataset

Let's have a look at the dataset first.

```{r}
data(bank, package = "lightgbm")

bank[1L:5L, c("y", "age", "balance")]

# Distribution of the response
table(bank$y)
```

## Training the model

Building a strong LightGBM model typically involves a little bit of hyperparameter tuning. To keep things as simple as possible, we skip this step here and directly jump into fitting a first model.

The R package of LightGBM offers two ways to do so:

- `lgb.train`: This is the main training logic. It offers full flexibility but requires data created by the `lgb.Dataset` function.
- `lightgbm`: Simpler, but less flexible. Data can be passed without having to bother with `lgb.Dataset`.


### Using the `lightgbm` function

In a first step, we will make sure that all data is numeric. Then, we will fit the model by the `lightgbm` function and check how the distribution of the predictions looks like. 

```{r}
# Numeric response and feature matrix
y <- as.numeric(bank$y == "yes")
X <- data.matrix(bank[, c("age", "balance")])

# Train
fit <- lightgbm(
  data = X
  , label = y
  , num_leaves = 4L
  , learning_rate = 1.0
  , nrounds = 10L
  , objective = "binary"
  , verbose = -1L
)

# Result
summary(predict(fit, X))
```

It seems to have worked! And we can see that the predictions are indeed probabilities between 0 and 1.

### Using the `lgb.train` function

Alternatively, we can go for the more flexible interface `lgb.train`. Here, as an additional step, we need to prepare `y` and `X` by the data API `lgb.Dataset` of LightGBM. Parameters are passed to `lgb.train` as a named list.

```{r}
# Data interface
dtrain <- lgb.Dataset(X, label = y)

# Parameters
params <- list(
  objective = "binary"
  , num_leaves = 4L
  , learning_rate = 1.0
)

# Train
fit <- lgb.train(
  params
  , data = dtrain
  , nrounds = 10L
  , verbose = -1L
)
```

Try it out! If you get stuck, visit LightGBM's [documentation](https://lightgbm.readthedocs.io/en/latest/R/index.html) for more details.

```{r, echo = FALSE, results = "hide"}
# Cleanup
if (file.exists("lightgbm.model")) {
  file.remove("lightgbm.model")
}
```

## References
